#!/usr/bin/env python3
"""
æ—¥ç¨‹ãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—ã—ã¦åˆ†æã™ã‚‹ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

from ..scrapers.requests_scraper import RequestsScraper
from ..parsers.nittei_parser import NitteiParser
from bs4 import BeautifulSoup
import os

def main():
    # RequestsScraperã§HTMLã‚’å–å¾—
    scraper = RequestsScraper()
    url = 'https://p.keibabook.co.jp/cyuou/nittei/20250607'
    
    print(f"ğŸ” æ—¥ç¨‹ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­: {url}")
    html_content = scraper.scrape(url)
    
    # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    os.makedirs('debug_html', exist_ok=True)
    html_file = 'debug_html/nittei_20250607.html'
    with open(html_file, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"âœ… HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: {html_file}")
    print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {len(html_content):,}æ–‡å­—")
    
    # BeautifulSoupã§è§£æ
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # é–‹å‚¬å ´æ‰€ã‚’æ¢ã™
    print("\nğŸ” HTMLæ§‹é€ ã‚’åˆ†æä¸­...")
    
    # æ§˜ã€…ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã§é–‹å‚¬å ´æ‰€ã‚’æ¢ã™
    patterns = [
        ('div.kaisai', 'div class="kaisai"'),
        ('table.kaisai', 'table class="kaisai"'),
        ('div[class*="kaisai"]', 'div with kaisai in class'),
        ('table[class*="kaisai"]', 'table with kaisai in class'),
        ('th.midasi', 'th class="midasi"'),
        ('th[class*="midasi"]', 'th with midasi in class'),
    ]
    
    for selector, description in patterns:
        elements = soup.select(selector)
        print(f"ğŸ“‹ {description}: {len(elements)}å€‹")
        if elements:
            for i, elem in enumerate(elements[:3]):  # æœ€åˆã®3å€‹ã ã‘è¡¨ç¤º
                text = elem.get_text(strip=True)[:100]  # æœ€åˆã®100æ–‡å­—
                print(f"   [{i+1}] {text}")
    
    # å…¨ã¦ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç¢ºèª
    tables = soup.find_all('table')
    print(f"\nğŸ“Š å…¨ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")
    
    for i, table in enumerate(tables[:5]):  # æœ€åˆã®5å€‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«
        print(f"\nğŸ ãƒ†ãƒ¼ãƒ–ãƒ« {i+1}:")
        rows = table.find_all('tr')
        print(f"   è¡Œæ•°: {len(rows)}")
        
        # æœ€åˆã®æ•°è¡Œã‚’è¡¨ç¤º
        for j, row in enumerate(rows[:3]):
            cells = row.find_all(['th', 'td'])
            cell_texts = [cell.get_text(strip=True)[:50] for cell in cells]
            print(f"   è¡Œ{j+1}: {cell_texts}")
    
    # NitteiParserã§ãƒ‘ãƒ¼ã‚¹
    print("\nğŸ”§ NitteiParserã§ãƒ‘ãƒ¼ã‚¹ä¸­...")
    parser = NitteiParser()
    result = parser.parse_with_date(html_content, '20250607')
    
    if result:
        print(f"âœ… ãƒ‘ãƒ¼ã‚¹æˆåŠŸ:")
        print(f"   é–‹å‚¬æ•°: {result.get('kaisai_count', 0)}")
        print(f"   ç·ãƒ¬ãƒ¼ã‚¹æ•°: {result.get('total_races', 0)}")
        
        kaisai_data = result.get('kaisai_data', {})
        for venue, races in kaisai_data.items():
            print(f"   ğŸ“ {venue}: {len(races)}ãƒ¬ãƒ¼ã‚¹")
            for race in races[:3]:  # æœ€åˆã®3ãƒ¬ãƒ¼ã‚¹ã‚’è¡¨ç¤º
                print(f"      - {race.get('race_no')}R: {race.get('race_name')} (ID: {race.get('race_id')})")
    else:
        print("âŒ ãƒ‘ãƒ¼ã‚¹å¤±æ•—")
    
    scraper.close()

if __name__ == '__main__':
    main() 