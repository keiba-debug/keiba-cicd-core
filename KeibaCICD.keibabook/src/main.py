#!/usr/bin/env python3
"""
ç«¶é¦¬ãƒ–ãƒƒã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«

ä½¿ç”¨æ–¹æ³•:
    python src/keibabook/main.py --race-id 202502041211 --mode scrape_and_parse
    python src/keibabook/main.py --race-id 202502041211 --mode parse_only --html-file data/debug/seiseki.html
    python src/keibabook/main.py --test
"""

import argparse
import sys
from pathlib import Path
from typing import List

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¨ã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent.parent  # keiba-cicd-coreãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
src_dir = Path(__file__).parent  # KeibaCICD.keibabook/src
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(src_dir))

# .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
try:
    from dotenv import load_dotenv
    env_path = project_root / ".env"
    if env_path.exists():
        load_dotenv(env_path)
        print(f"[INFO] .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {env_path}")
except ImportError:
    print("[WARNING] python-dotenvãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

from .scrapers.keibabook_scraper import KeibabookScraper
from .scrapers.requests_scraper import RequestsScraper
from .parsers.seiseki_parser import SeisekiParser
from .parsers.syutuba_parser import SyutubaParser
from .parsers.cyokyo_parser import CyokyoParser
from .parsers.danwa_parser import DanwaParser
from .utils.config import Config
from .utils.logger import setup_logger
from .batch.core.common import get_json_file_path, ensure_batch_directories
# from batch_processor import BatchProcessor  # å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«
# from simple_batch import SimpleBatchProcessor  # å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«


def scrape_and_parse(race_id: str, save_html: bool = True, use_requests: bool = False) -> bool:
    """
    ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¨ãƒ‘ãƒ¼ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹
    
    Args:
        race_id: ãƒ¬ãƒ¼ã‚¹ID
        save_html: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
        use_requests: requestsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆChromeãªã—ï¼‰
        
    Returns:
        bool: æˆåŠŸã—ãŸå ´åˆTrue
    """
    logger = setup_logger("main", level="INFO")
    
    try:
        # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå¾“æ¥ + æ–°ä¿å­˜å…ˆï¼‰
        Config.ensure_directories()
        ensure_batch_directories()
        
        logger.info(f"ãƒ¬ãƒ¼ã‚¹ID {race_id} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’é–‹å§‹ã—ã¾ã™")
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        logger.info("=== ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° ===")
        
        if use_requests:
            logger.info("requestsãƒ™ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™")
            scraper = RequestsScraper(debug=Config.get_debug_mode())
        else:
            logger.info("Seleniumãƒ™ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™")
            scraper = KeibabookScraper(
                headless=Config.get_headless_mode(),
                debug=Config.get_debug_mode()
            )
        
        html_file_path = None
        if save_html:
            html_file_path = Config.get_debug_dir() / f"seiseki_{race_id}_scraped.html"
        
        html_content = scraper.scrape_seiseki_page(race_id, str(html_file_path) if html_file_path else None)
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œè¨¼
        if not scraper.validate_page_content(html_content):
            logger.error("å–å¾—ã—ãŸHTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
            return False
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‘ãƒ¼ã‚¹
        logger.info("=== ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ¼ã‚¹ ===")
        parser = SeisekiParser(debug=Config.get_debug_mode())
        
        if html_file_path and html_file_path.exists():
            data = parser.parse(str(html_file_path))
        else:
            # HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ä¸€æ™‚çš„ã«ä¿å­˜
            temp_html_path = Config.get_debug_dir() / f"temp_seiseki_{race_id}.html"
            with open(temp_html_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            data = parser.parse(str(temp_html_path))
            temp_html_path.unlink()  # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        
        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        if not parser.validate_data(data):
            logger.error("æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒä¸æ­£ã§ã™")
            return False
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: çµæœã®ä¿å­˜ï¼ˆä¿å­˜å…ˆã‚’ KEIBA_DATA_ROOT_DIR ã«çµ±ä¸€ï¼‰
        logger.info("=== çµæœã®ä¿å­˜ ===")
        output_path = Path(get_json_file_path('seiseki', race_id))
        parser.save_json(data, str(output_path))
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        results = data.get("results", [])
        interview_count = sum(1 for result in results if result.get("interview", "").strip())
        memo_count = sum(1 for result in results if result.get("memo", "").strip())
        
        logger.info(f"âœ… å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        logger.info(f"å‡ºèµ°é ­æ•°: {len(results)}é ­")
        logger.info(f"ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼æœ‰ã‚Š: {interview_count}é ­")
        logger.info(f"ãƒ¡ãƒ¢æœ‰ã‚Š: {memo_count}é ­")
        logger.info(f"ä¿å­˜å…ˆ: {output_path}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


def parse_only(html_file_path: str, race_id: str = None) -> bool:
    """
    HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ãƒ¼ã‚¹ã®ã¿ã‚’å®Ÿè¡Œã™ã‚‹
    
    Args:
        html_file_path: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        race_id: ãƒ¬ãƒ¼ã‚¹IDï¼ˆå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆã«ä½¿ç”¨ï¼‰
        
    Returns:
        bool: æˆåŠŸã—ãŸå ´åˆTrue
    """
    logger = setup_logger("main", level="INFO")
    
    try:
        # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå¾“æ¥ + æ–°ä¿å­˜å…ˆï¼‰
        Config.ensure_directories()
        ensure_batch_directories()
        
        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        html_path = Path(html_file_path)
        if not html_path.exists():
            logger.error(f"HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {html_file_path}")
            return False
        
        logger.info(f"HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ãƒ¼ã‚¹ã‚’é–‹å§‹ã—ã¾ã™: {html_file_path}")
        
        # ãƒ‘ãƒ¼ã‚¹å®Ÿè¡Œ
        parser = SeisekiParser(debug=Config.get_debug_mode())
        data = parser.parse(str(html_path))
        
        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        if not parser.validate_data(data):
            logger.error("æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒä¸æ­£ã§ã™")
            return False
        
        # çµæœã®ä¿å­˜
        identifier = race_id if race_id else html_path.stem
        output_path = Path(get_json_file_path('seiseki', identifier))
        parser.save_json(data, str(output_path))
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        results = data.get("results", [])
        interview_count = sum(1 for result in results if result.get("interview", "").strip())
        memo_count = sum(1 for result in results if result.get("memo", "").strip())
        
        logger.info(f"âœ… ãƒ‘ãƒ¼ã‚¹ãŒå®Œäº†ã—ã¾ã—ãŸ")
        logger.info(f"å‡ºèµ°é ­æ•°: {len(results)}é ­")
        logger.info(f"ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼æœ‰ã‚Š: {interview_count}é ­")
        logger.info(f"ãƒ¡ãƒ¢æœ‰ã‚Š: {memo_count}é ­")
        logger.info(f"ä¿å­˜å…ˆ: {output_path}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


def batch_process_date_range(from_date_str: str, to_date_str: str, 
                           use_requests: bool = True, save_html: bool = True) -> bool:
    """
    é–‹å‚¬æ—¥ç¯„å›²ã§ã®å…¨ãƒ¬ãƒ¼ã‚¹å‡¦ç†
    
    Args:
        from_date_str: é–‹å§‹æ—¥ (YYYY-MM-DDå½¢å¼)
        to_date_str: çµ‚äº†æ—¥ (YYYY-MM-DDå½¢å¼)
        use_requests: requestsã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        save_html: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‹
        
    Returns:
        bool: æˆåŠŸã—ãŸå ´åˆTrue
    """
    logger = setup_logger("batch_main", level="INFO")
    
    try:
        # æ—¥ä»˜ã®ãƒ‘ãƒ¼ã‚¹
        from datetime import datetime
        from_date = datetime.strptime(from_date_str, '%Y-%m-%d').date()
        to_date = datetime.strptime(to_date_str, '%Y-%m-%d').date()
        
        if from_date > to_date:
            logger.error("é–‹å§‹æ—¥ãŒçµ‚äº†æ—¥ã‚ˆã‚Šå¾Œã«ãªã£ã¦ã„ã¾ã™")
            return False
        
        logger.info(f"ğŸ‡ é–‹å‚¬æ—¥ç¯„å›²ãƒãƒƒãƒå‡¦ç†é–‹å§‹")
        logger.info(f"ğŸ“… æœŸé–“: {from_date} ï½ {to_date}")
        logger.info(f"ğŸ”§ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼: {'requests' if use_requests else 'Selenium'}")
        
        # ãƒãƒƒãƒãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼åˆæœŸåŒ–
        processor = BatchProcessor(use_requests=use_requests, debug=Config.get_debug_mode())
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¬ãƒ¼ã‚¹IDå–å¾—
        logger.info("=" * 50)
        logger.info("ğŸ” ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¬ãƒ¼ã‚¹IDå–å¾—")
        logger.info("=" * 50)
        
        race_ids = processor.get_race_ids_for_date_range(from_date, to_date)
        
        if not race_ids:
            logger.warning("æŒ‡å®šæœŸé–“ã«ãƒ¬ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return True
        
        logger.info(f"ğŸ“Š å–å¾—ã—ãŸãƒ¬ãƒ¼ã‚¹æ•°: {len(race_ids)}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: å…¨ãƒ¬ãƒ¼ã‚¹å‡¦ç†
        logger.info("=" * 50)
        logger.info("âš™ï¸ ã‚¹ãƒ†ãƒƒãƒ—2: å…¨ãƒ¬ãƒ¼ã‚¹å‡¦ç†")
        logger.info("=" * 50)
        
        results = processor.process_all_races(race_ids, save_html=save_html)
        
        # çµæœè¡¨ç¤º
        stats = results['stats']
        logger.info("ğŸ‰ ãƒãƒƒãƒå‡¦ç†å®Œäº†ï¼")
        logger.info(f"âœ… æˆåŠŸ: {stats['success_count']}/{stats['total_races']} ãƒ¬ãƒ¼ã‚¹")
        
        if stats['error_count'] > 0:
            logger.warning(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {stats['error_count']} ãƒ¬ãƒ¼ã‚¹")
        
        if stats['skipped_count'] > 0:
            logger.info(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {stats['skipped_count']} ãƒ¬ãƒ¼ã‚¹ (æ—¢å‡¦ç†æ¸ˆã¿)")
        
        return stats['error_count'] == 0
        
    except ValueError as e:
        logger.error(f"æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        logger.error("æ—¥ä»˜ã¯ YYYY-MM-DD å½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ (ä¾‹: 2025-02-01)")
        return False
    except Exception as e:
        logger.error(f"âŒ ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


def scrape_and_parse_multi_type(race_id: str, data_types: List[str], 
                               save_html: bool = True, use_requests: bool = False) -> bool:
    """
    è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’ä¸€æ‹¬ã§å–å¾—ãƒ»ãƒ‘ãƒ¼ã‚¹
    
    Args:
        race_id: ãƒ¬ãƒ¼ã‚¹ID
        data_types: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆ (['seiseki', 'syutuba', 'cyokyo', 'danwa'])
        save_html: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
        use_requests: requestsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
        
    Returns:
        bool: æˆåŠŸã—ãŸå ´åˆTrue
    """
    logger = setup_logger("multi_main", level="INFO")
    
    try:
        # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå¾“æ¥ + æ–°ä¿å­˜å…ˆï¼‰
        Config.ensure_directories()
        ensure_batch_directories()
        
        logger.info(f"ğŸ‡ è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—å‡¦ç†é–‹å§‹: ãƒ¬ãƒ¼ã‚¹ID {race_id}")
        logger.info(f"ğŸ“Š å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—: {', '.join(data_types)}")
        
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–
        if use_requests:
            logger.info("requestsãƒ™ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™")
            scraper = RequestsScraper(debug=Config.get_debug_mode())
        else:
            logger.info("Seleniumãƒ™ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™")
            scraper = KeibabookScraper(
                headless=Config.get_headless_mode(),
                debug=Config.get_debug_mode()
            )
        
        # ãƒ‘ãƒ¼ã‚µãƒ¼è¾æ›¸
        parsers = {
            'seiseki': SeisekiParser(debug=Config.get_debug_mode()),
            'syutuba': SyutubaParser(debug=Config.get_debug_mode()),
            'cyokyo': CyokyoParser(debug=Config.get_debug_mode()),
            'danwa': DanwaParser(debug=Config.get_debug_mode())
        }
        
        results = {}
        
        for data_type in data_types:
            logger.info(f"=" * 50)
            logger.info(f"ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—: {data_type}")
            logger.info(f"=" * 50)
            
            try:
                # HTMLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
                html_file_path = None
                if save_html:
                    html_file_path = Config.get_debug_dir() / f"{data_type}_{race_id}_scraped.html"
                
                # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                html_content = scraper.scrape_page(data_type, race_id, str(html_file_path) if html_file_path else None)
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œè¨¼
                if not scraper.validate_page_content(html_content):
                    logger.warning(f"{data_type}: HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
                    results[data_type] = False
                    continue
                
                # ãƒ‘ãƒ¼ã‚¹
                parser = parsers[data_type]
                if html_file_path and html_file_path.exists():
                    data = parser.parse(str(html_file_path))
                else:
                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã—ã¦ãƒ‘ãƒ¼ã‚¹
                    temp_html_path = Config.get_debug_dir() / f"temp_{data_type}_{race_id}.html"
                    with open(temp_html_path, 'w', encoding='utf-8') as f:
                        f.write(html_content)
                    data = parser.parse(str(temp_html_path))
                    temp_html_path.unlink()
                
                # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
                if not parser.validate_data(data):
                    logger.warning(f"{data_type}: æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒä¸æ­£ã§ã™")
                    results[data_type] = False
                    continue
                
                # ä¿å­˜ï¼ˆä¿å­˜å…ˆã‚’ KEIBA_DATA_ROOT_DIR ã«çµ±ä¸€ï¼‰
                output_path = Path(get_json_file_path(data_type, race_id))
                parser.save_json(data, str(output_path))
                
                logger.info(f"âœ… {data_type}: ä¿å­˜å®Œäº† - {output_path}")
                results[data_type] = True
                
            except Exception as e:
                logger.error(f"âŒ {data_type}: ã‚¨ãƒ©ãƒ¼ - {e}")
                results[data_type] = False
        
        # çµæœã‚µãƒãƒªãƒ¼
        logger.info("=" * 50)
        logger.info("ğŸ“Š å‡¦ç†çµæœã‚µãƒãƒªãƒ¼")
        logger.info("=" * 50)
        
        success_count = sum(1 for success in results.values() if success)
        total_count = len(data_types)
        
        for data_type, success in results.items():
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±æ•—"
            logger.info(f"  - {data_type}: {status}")
        
        logger.info(f"ğŸ¯ æˆåŠŸç‡: {success_count}/{total_count} ({(success_count/total_count)*100:.1f}%)")
        
        return success_count == total_count
        
    except Exception as e:
        logger.error(f"âŒ è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


def simple_batch_process_date_range(from_date_str: str, to_date_str: str, 
                                   use_requests: bool = True, save_html: bool = True) -> bool:
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒƒãƒå‡¦ç†ã§ã®æ—¥ä»˜ç¯„å›²å‡¦ç†
    
    Args:
        from_date_str: é–‹å§‹æ—¥ (YYYY-MM-DDå½¢å¼)
        to_date_str: çµ‚äº†æ—¥ (YYYY-MM-DDå½¢å¼)
        use_requests: requestsã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        save_html: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‹
        
    Returns:
        bool: æˆåŠŸã—ãŸå ´åˆTrue
    """
    logger = setup_logger("simple_batch_main", level="INFO")
    
    try:
        logger.info(f"ğŸ‡ ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒƒãƒå‡¦ç†é–‹å§‹")
        logger.info(f"ğŸ“… æœŸé–“: {from_date_str} ï½ {to_date_str}")
        logger.info(f"ğŸ”§ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼: {'requests' if use_requests else 'Selenium'}")
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒƒãƒãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼åˆæœŸåŒ–
        processor = SimpleBatchProcessor(use_requests=use_requests, debug=Config.get_debug_mode())
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¬ãƒ¼ã‚¹IDç”Ÿæˆ
        logger.info("=" * 50)
        logger.info("ğŸ”¢ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¬ãƒ¼ã‚¹IDç”Ÿæˆ")
        logger.info("=" * 50)
        
        race_ids = processor.generate_race_ids_for_date_range(from_date_str, to_date_str)
        
        if not race_ids:
            logger.warning("ãƒ¬ãƒ¼ã‚¹IDãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return True
        
        logger.info(f"ğŸ“Š ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒ¼ã‚¹æ•°: {len(race_ids)}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: æœ€åˆã®æ•°ä»¶ã§ãƒ†ã‚¹ãƒˆ
        test_race_ids = race_ids[:5]  # æœ€åˆã®5ä»¶ã§ãƒ†ã‚¹ãƒˆ
        logger.info(f"ğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: æœ€åˆã® {len(test_race_ids)} ä»¶")
        for test_id in test_race_ids:
            logger.info(f"  - {test_id}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: å®Ÿéš›ã®å‡¦ç†
        logger.info("=" * 50)
        logger.info("âš™ï¸ ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¬ãƒ¼ã‚¹å‡¦ç†")
        logger.info("=" * 50)
        
        results = processor.process_race_ids(test_race_ids, save_html=save_html, max_errors=3)
        
        # çµæœè¡¨ç¤º
        stats = results['stats']
        logger.info("ğŸ‰ ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒƒãƒå‡¦ç†å®Œäº†ï¼")
        logger.info(f"âœ… æˆåŠŸ: {stats['success_count']}/{stats['total_races']} ãƒ¬ãƒ¼ã‚¹")
        
        if stats['error_count'] > 0:
            logger.warning(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {stats['error_count']} ãƒ¬ãƒ¼ã‚¹")
        
        if stats['skipped_count'] > 0:
            logger.info(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {stats['skipped_count']} ãƒ¬ãƒ¼ã‚¹ (æ—¢å‡¦ç†æ¸ˆã¿)")
        
        return stats['error_count'] == 0
        
    except Exception as e:
        logger.error(f"âŒ ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


def run_tests() -> bool:
    """
    ç°¡å˜ãªå‹•ä½œãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹
    
    Returns:
        bool: æˆåŠŸã—ãŸå ´åˆTrue
    """
    logger = setup_logger("main", level="INFO")
    
    try:
        logger.info("ã‚·ã‚¹ãƒ†ãƒ å‹•ä½œãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™")
        
        # åŸºæœ¬çš„ãªã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
        tests = [
            ("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿", test_config_loading),
            ("ãƒ‘ãƒ¼ã‚µãƒ¼åˆæœŸåŒ–", test_parsers_initialization),
            ("ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ", test_directory_creation),
            ("åŸºæœ¬æ©Ÿèƒ½ãƒã‚§ãƒƒã‚¯", test_basic_functionality)
        ]
        
        results = []
        for test_name, test_func in tests:
            logger.info(f"å®Ÿè¡Œä¸­: {test_name}")
            try:
                result = test_func()
                results.append((test_name, result))
                if result:
                    logger.info(f"âœ… {test_name}: æˆåŠŸ")
                else:
                    logger.error(f"âŒ {test_name}: å¤±æ•—")
            except Exception as e:
                logger.error(f"âŒ {test_name}: ã‚¨ãƒ©ãƒ¼ - {e}")
                results.append((test_name, False))
        
        # çµæœã®ã‚µãƒãƒªãƒ¼
        success_count = sum(1 for _, result in results if result)
        total_count = len(results)
        
        logger.info(f"ãƒ†ã‚¹ãƒˆçµæœ: {success_count}/{total_count} æˆåŠŸ")
        
        return success_count == total_count
        
    except Exception as e:
        logger.error(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False


def test_config_loading() -> bool:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
    try:
        Config.ensure_directories()
        return True
    except Exception as e:
        print(f"è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        return False


def test_parsers_initialization() -> bool:
    """ãƒ‘ãƒ¼ã‚µãƒ¼åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
    try:
        from .parsers.seiseki_parser import SeisekiParser
        from .parsers.syutuba_parser import SyutubaParser
        
        seiseki_parser = SeisekiParser(debug=True)
        syutuba_parser = SyutubaParser(debug=True)
        
        return True
    except Exception as e:
        print(f"ãƒ‘ãƒ¼ã‚µãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def test_directory_creation() -> bool:
    """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆãƒ†ã‚¹ãƒˆ"""
    try:
        data_dir = Config.get_data_dir()
        debug_dir = Config.get_debug_dir()
        seiseki_dir = Config.get_seiseki_dir()
        
        return data_dir.exists() and debug_dir.exists() and seiseki_dir.exists()
    except Exception as e:
        print(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚¨ãƒ©ãƒ¼: {e}")
        return False


def test_basic_functionality() -> bool:
    """åŸºæœ¬æ©Ÿèƒ½ãƒã‚§ãƒƒã‚¯"""
    try:
        from .utils.logger import setup_logger
        test_logger = setup_logger("test", level="INFO")
        test_logger.info("åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
        return True
    except Exception as e:
        print(f"åŸºæœ¬æ©Ÿèƒ½ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def main():
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    parser = argparse.ArgumentParser(description="ç«¶é¦¬ãƒ–ãƒƒã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--race-id", type=str, help="ãƒ¬ãƒ¼ã‚¹ID")
    parser.add_argument("--mode", type=str, choices=["scrape_and_parse", "parse_only", "multi_type", "batch", "simple_batch"], 
                       default="scrape_and_parse", help="å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--html-file", type=str, help="HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆparse_onlyãƒ¢ãƒ¼ãƒ‰ç”¨ï¼‰")
    parser.add_argument("--from-date", type=str, help="é–‹å§‹æ—¥ (YYYY-MM-DDå½¢å¼ã€batchãƒ¢ãƒ¼ãƒ‰ç”¨)")
    parser.add_argument("--to-date", type=str, help="çµ‚äº†æ—¥ (YYYY-MM-DDå½¢å¼ã€batchãƒ¢ãƒ¼ãƒ‰ç”¨)")
    parser.add_argument("--test", action="store_true", help="ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ")
    parser.add_argument("--no-save-html", action="store_true", help="HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ãªã„")
    parser.add_argument("--use-requests", action="store_true", help="requestsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ï¼ˆChromeãªã—ï¼‰")
    parser.add_argument("--data-types", type=str, nargs='+', 
                       choices=['seiseki', 'syutuba', 'cyokyo', 'danwa'],
                       default=['seiseki'],
                       help="å–å¾—ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ— (è¤‡æ•°æŒ‡å®šå¯èƒ½ã€multi_typeãƒ¢ãƒ¼ãƒ‰ç”¨)")
    
    args = parser.parse_args()
    
    if args.test:
        success = run_tests()
        sys.exit(0 if success else 1)
    
    if args.mode == "batch":
        # ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
        if not args.from_date or not args.to_date:
            parser.error("--from-date ã¨ --to-date ãŒå¿…è¦ã§ã™ï¼ˆbatch ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        success = batch_process_date_range(
            args.from_date, 
            args.to_date, 
            use_requests=args.use_requests, 
            save_html=not args.no_save_html
        )
    elif args.mode == "simple_batch":
        # ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
        if not args.from_date or not args.to_date:
            parser.error("--from-date ã¨ --to-date ãŒå¿…è¦ã§ã™ï¼ˆsimple_batch ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        success = simple_batch_process_date_range(
            args.from_date, 
            args.to_date, 
            use_requests=args.use_requests, 
            save_html=not args.no_save_html
        )
    elif args.mode == "scrape_and_parse":
        # å˜ä¸€ãƒ¬ãƒ¼ã‚¹å‡¦ç†
        if not args.race_id:
            parser.error("--race-id ãŒå¿…è¦ã§ã™ï¼ˆscrape_and_parse ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        success = scrape_and_parse(args.race_id, save_html=not args.no_save_html, use_requests=args.use_requests)
    elif args.mode == "parse_only":
        # ãƒ‘ãƒ¼ã‚¹ã®ã¿
        if not args.html_file:
            parser.error("--html-file ãŒå¿…è¦ã§ã™ï¼ˆparse_only ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        if not args.race_id:
            parser.error("--race-id ãŒå¿…è¦ã§ã™ï¼ˆparse_only ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        success = parse_only(args.html_file, args.race_id)
    elif args.mode == "multi_type":
        # è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—å‡¦ç†
        if not args.race_id:
            parser.error("--race-id ãŒå¿…è¦ã§ã™ï¼ˆmulti_type ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        success = scrape_and_parse_multi_type(
            args.race_id, 
            args.data_types, 
            save_html=not args.no_save_html, 
            use_requests=args.use_requests
        )
    else:
        parser.error("ç„¡åŠ¹ãªãƒ¢ãƒ¼ãƒ‰ã§ã™")
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()