#!/usr/bin/env python3
"""
æœ€é©åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆrequestsç‰ˆï¼‰

ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ç‰ˆã®DataFetcher
- RequestsScraperã‚’ä½¿ç”¨ï¼ˆSeleniumä¸ä½¿ç”¨ï¼‰
- ä¸¦åˆ—å‡¦ç†å¯¾å¿œ
- ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
- å¤§å¹…ãªé«˜é€ŸåŒ–
"""

import os
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

from ..scrapers.requests_scraper import RequestsScraper
from ..parsers.seiseki_parser import SeisekiParser
from ..parsers.syutuba_parser import SyutubaParser
from ..parsers.cyokyo_parser import CyokyoParser
from ..parsers.danwa_parser import DanwaParser
from ..parsers.nittei_parser import NitteiParser
from .core.common import ensure_batch_directories, get_race_ids_file_path, get_json_file_path


class OptimizedDataFetcher:
    """
    æœ€é©åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¹ï¼ˆrequestsç‰ˆï¼‰
    
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„:
    1. RequestsScraperã‚’ä½¿ç”¨ï¼ˆSeleniumä¸ä½¿ç”¨ï¼‰
    2. ä¸¦åˆ—å‡¦ç†å¯¾å¿œ
    3. ã‚»ãƒƒã‚·ãƒ§ãƒ³å†åˆ©ç”¨
    4. ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
    5. å¤§å¹…ãªé«˜é€ŸåŒ–ï¼ˆ10-20å€ã®é€Ÿåº¦å‘ä¸Šï¼‰
    """
    
    def __init__(self, delay: int = 1, max_workers: int = 5):
        """
        åˆæœŸåŒ–
        
        Args:
            delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ï¼‰- requestsãªã®ã§çŸ­ç¸®å¯èƒ½
            max_workers: ä¸¦åˆ—å‡¦ç†ã®æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        """
        self.delay = delay
        self.max_workers = max_workers
        
        # RequestsScraperã‚’ä½¿ç”¨ï¼ˆè»½é‡ãƒ»é«˜é€Ÿï¼‰
        self.scraper = RequestsScraper()
        
        # ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’åˆæœŸåŒ–ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰
        self.seiseki_parser = SeisekiParser()
        self.shutsuba_parser = SyutubaParser()
        self.cyokyo_parser = CyokyoParser()
        self.danwa_parser = DanwaParser()
        self.nittei_parser = NitteiParser()
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        ensure_batch_directories()
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info("ğŸš€ æœ€é©åŒ–DataFetcherï¼ˆrequestsç‰ˆï¼‰ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    
    def parse_html_content_direct(self, html_content: str, data_type: str, race_id: str) -> Optional[Dict[str, Any]]:
        """
        HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç›´æ¥ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ï¼ˆä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä¸ä½¿ç”¨ï¼‰
        
        Args:
            html_content: HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            data_type: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—
            race_id: ãƒ¬ãƒ¼ã‚¹ID
            
        Returns:
            Optional[Dict[str, Any]]: ãƒ‘ãƒ¼ã‚¹çµæœ
        """
        try:
            # BeautifulSoupã§ç›´æ¥ãƒ‘ãƒ¼ã‚¹
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            if data_type == 'seiseki':
                # SeisekiParserã®å†…éƒ¨ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç›´æ¥ä½¿ç”¨
                race_info = self.seiseki_parser._extract_race_info(soup)
                results = self.seiseki_parser._extract_results(soup)
                interviews_and_memos = self.seiseki_parser._extract_interviews_and_memos(soup)
                results = self.seiseki_parser._merge_interview_memo_data(results, interviews_and_memos)
                return {"race_info": race_info, "results": results}
                
            elif data_type == 'shutsuba':
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã‚ãšã«ç›´æ¥ãƒ‘ãƒ¼ã‚¹
                import tempfile
                with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False, encoding='utf-8') as tmp_file:
                    tmp_file.write(html_content)
                    tmp_file_path = tmp_file.name
                try:
                    return self.shutsuba_parser.parse(tmp_file_path)
                finally:
                    os.unlink(tmp_file_path)
                    
            elif data_type == 'cyokyo':
                import tempfile
                with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False, encoding='utf-8') as tmp_file:
                    tmp_file.write(html_content)
                    tmp_file_path = tmp_file.name
                try:
                    return self.cyokyo_parser.parse(tmp_file_path)
                finally:
                    os.unlink(tmp_file_path)
                    
            elif data_type == 'danwa':
                import tempfile
                with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False, encoding='utf-8') as tmp_file:
                    tmp_file.write(html_content)
                    tmp_file_path = tmp_file.name
                try:
                    return self.danwa_parser.parse(tmp_file_path)
                finally:
                    os.unlink(tmp_file_path)
                    
        except Exception as e:
            self.logger.error(f"âŒ {data_type}ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def fetch_single_race_data_fast(self, race_id: str, data_type: str) -> bool:
        """
        å˜ä¸€ãƒ¬ãƒ¼ã‚¹ã®å˜ä¸€ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’é«˜é€Ÿå–å¾—ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰
        
        Args:
            race_id: ãƒ¬ãƒ¼ã‚¹ID
            data_type: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—
            
        Returns:
            bool: æˆåŠŸã—ãŸå ´åˆTrue
        """
        try:
            self.logger.info(f"âš¡ {data_type}ãƒ‡ãƒ¼ã‚¿é«˜é€Ÿå–å¾—é–‹å§‹: {race_id}")
            
            # RequestsScraperã§é«˜é€Ÿãƒ‡ãƒ¼ã‚¿å–å¾—
            html_content = None
            if data_type == 'seiseki':
                html_content = self.scraper.scrape_seiseki_page(race_id)
            elif data_type == 'shutsuba':
                html_content = self.scraper.scrape_syutuba_page(race_id)
            elif data_type == 'cyokyo':
                html_content = self.scraper.scrape_cyokyo_page(race_id)
            elif data_type == 'danwa':
                html_content = self.scraper.scrape_danwa_page(race_id)
            
            if not html_content:
                self.logger.error(f"âŒ {data_type}ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {race_id}")
                return False
            
            # ãƒ‘ãƒ¼ã‚¹å‡¦ç†ï¼ˆä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä¸ä½¿ç”¨ï¼‰
            parsed_data = self.parse_html_content_direct(html_content, data_type, race_id)
            
            if not parsed_data:
                self.logger.error(f"âŒ {data_type}ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {race_id}")
                return False
            
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            json_file_path = get_json_file_path(data_type, race_id)
            with open(json_file_path, 'w', encoding='utf-8') as f:
                json.dump(parsed_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"âœ… {data_type} JSONä¿å­˜å®Œäº†: {json_file_path}")
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆrequestsãªã®ã§çŸ­ç¸®å¯èƒ½ï¼‰
            if self.delay > 0:
                time.sleep(self.delay)
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {data_type}ãƒ‡ãƒ¼ã‚¿å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def fetch_race_schedule_fast(self, date_str: str) -> bool:
        """
        ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹ã‚’é«˜é€Ÿå–å¾—
        é–‹å‚¬ãŒãªã„æ—¥ã¯JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›ã—ãªã„
        
        Args:
            date_str: æ—¥ä»˜æ–‡å­—åˆ— (YYYYMMDD)
            
        Returns:
            bool: æˆåŠŸã—ãŸå ´åˆTrueï¼ˆé–‹å‚¬ãŒãªã„æ—¥ã‚‚Trueã‚’è¿”ã™ï¼‰
        """
        try:
            self.logger.info(f"âš¡ ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹é«˜é€Ÿå–å¾—é–‹å§‹: {date_str}")
            
            # RequestsScraperã§ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹ã‚’å–å¾—
            url = f"https://p.keibabook.co.jp/cyuou/nittei/{date_str}"
            html_content = self.scraper.scrape(url)
            
            if not html_content:
                self.logger.error(f"âŒ ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {date_str}")
                return False
            
            # NitteiParserã§ãƒ‘ãƒ¼ã‚¹
            parsed_data = self.nittei_parser.parse_with_date(html_content, date_str)
            
            if not parsed_data:
                self.logger.error(f"âŒ ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {date_str}")
                return False
            
            # é–‹å‚¬ãŒãªã„æ—¥ã®åˆ¤å®š
            total_races = parsed_data.get('total_races', 0)
            kaisai_count = parsed_data.get('kaisai_count', 0)
            
            if total_races == 0 or kaisai_count == 0:
                self.logger.info(f"ğŸ“­ é–‹å‚¬ãªã—: {date_str} - ãƒ¬ãƒ¼ã‚¹æ•°: {total_races}, é–‹å‚¬æ•°: {kaisai_count}")
                self.logger.info(f"â­ï¸ JSONãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡ºåŠ›ã—ã¾ã›ã‚“")
                return True  # é–‹å‚¬ãŒãªã„æ—¥ã‚‚æ­£å¸¸å‡¦ç†ã¨ã—ã¦æ‰±ã†
            
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            json_file_path = get_json_file_path('nittei', date_str)
            with open(json_file_path, 'w', encoding='utf-8') as f:
                json.dump(parsed_data, f, ensure_ascii=False, indent=2)
            
            # ãƒ¬ãƒ¼ã‚¹IDæƒ…å ±ã‚‚ä¿å­˜
            race_ids_file = get_race_ids_file_path(date_str)
            race_ids_data = {
                'date': date_str,
                'kaisai_data': {}
            }
            
            # ãƒ¬ãƒ¼ã‚¹IDã‚’æŠ½å‡º
            kaisai_data = parsed_data.get('kaisai_data', {})
            for venue, races in kaisai_data.items():
                race_ids_data['kaisai_data'][venue] = races
            
            with open(race_ids_file, 'w', encoding='utf-8') as f:
                json.dump(race_ids_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"âœ… ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹é«˜é€Ÿå–å¾—å®Œäº†: {json_file_path}")
            self.logger.info(f"ğŸ‡ é–‹å‚¬æƒ…å ±: {kaisai_count}é–‹å‚¬, {total_races}ãƒ¬ãƒ¼ã‚¹")
            self.logger.info(f"âœ… ãƒ¬ãƒ¼ã‚¹IDæƒ…å ±ä¿å­˜å®Œäº†: {race_ids_file}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def fetch_all_race_data_parallel_fast(self, date_str: str, data_types: List[str]) -> Dict[str, Any]:
        """
        ä¸¦åˆ—å‡¦ç†ã§å…¨ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’é«˜é€Ÿå–å¾—
        
        Args:
            date_str: æ—¥ä»˜æ–‡å­—åˆ— (YYYYMMDD)
            data_types: å–å¾—ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            Dict[str, Any]: å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼
        """
        start_time = time.time()
        
        self.logger.info(f"ğŸš€ ä¸¦åˆ—é«˜é€Ÿãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: {date_str}")
        self.logger.info(f"ğŸ“Š å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—: {', '.join(data_types)}")
        self.logger.info(f"âš¡ æœ€å¤§ä¸¦åˆ—æ•°: {self.max_workers}")
        self.logger.info(f"ğŸ”¥ RequestsScraperã‚’ä½¿ç”¨ï¼ˆSeleniumä¸ä½¿ç”¨ï¼‰")
        
        # ãƒ¬ãƒ¼ã‚¹IDã‚’å–å¾—
        race_ids = self.get_race_ids_from_file(date_str)
        if not race_ids:
            self.logger.error(f"âŒ ãƒ¬ãƒ¼ã‚¹IDãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {date_str}")
            return {'success': False, 'error': 'No race IDs found'}
        
        # ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        tasks = []
        for race_id in race_ids:
            for data_type in data_types:
                tasks.append((race_id, data_type))
        
        self.logger.info(f"ğŸ“‹ ç·ã‚¿ã‚¹ã‚¯æ•°: {len(tasks)}ä»¶ï¼ˆ{len(race_ids)}ãƒ¬ãƒ¼ã‚¹ Ã— {len(data_types)}ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ï¼‰")
        
        # ä¸¦åˆ—å‡¦ç†ã§å®Ÿè¡Œ
        total_success = 0
        total_failed = 0
        results_by_type = {data_type: {'success': 0, 'failed': 0} for data_type in data_types}
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
            future_to_task = {
                executor.submit(self.fetch_single_race_data_fast, race_id, data_type): (race_id, data_type)
                for race_id, data_type in tasks
            }
            
            # çµæœã‚’åé›†
            completed_tasks = 0
            for future in as_completed(future_to_task):
                race_id, data_type = future_to_task[future]
                completed_tasks += 1
                
                try:
                    success = future.result()
                    if success:
                        results_by_type[data_type]['success'] += 1
                        total_success += 1
                    else:
                        results_by_type[data_type]['failed'] += 1
                        total_failed += 1
                        
                    # é€²æ—è¡¨ç¤º
                    if completed_tasks % 10 == 0 or completed_tasks == len(tasks):
                        progress = (completed_tasks / len(tasks)) * 100
                        elapsed = time.time() - start_time
                        self.logger.info(f"ğŸ“ˆ é€²æ—: {completed_tasks}/{len(tasks)} ({progress:.1f}%) - çµŒéæ™‚é–“: {elapsed:.1f}ç§’")
                        
                except Exception as e:
                    self.logger.error(f"âŒ ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({race_id}, {data_type}): {e}")
                    results_by_type[data_type]['failed'] += 1
                    total_failed += 1
        
        # å‡¦ç†æ™‚é–“è¨ˆç®—
        total_time = time.time() - start_time
        
        # çµæœã‚µãƒãƒªãƒ¼
        summary = {
            'success': True,
            'date': date_str,
            'total_races': len(race_ids),
            'total_tasks': len(tasks),
            'total_success': total_success,
            'total_failed': total_failed,
            'results_by_type': results_by_type,
            'processing_time_seconds': round(total_time, 2),
            'tasks_per_second': round(len(tasks) / total_time, 2) if total_time > 0 else 0
        }
        
        self.logger.info(f"âœ… ä¸¦åˆ—é«˜é€Ÿãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")
        self.logger.info(f"ğŸ“Š æˆåŠŸ: {total_success}ä»¶, å¤±æ•—: {total_failed}ä»¶")
        self.logger.info(f"â±ï¸ å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
        self.logger.info(f"ğŸš€ å‡¦ç†é€Ÿåº¦: {summary['tasks_per_second']:.2f}ã‚¿ã‚¹ã‚¯/ç§’")
        
        return summary
    
    def get_race_ids_from_file(self, date_str: str) -> List[str]:
        """
        ä¿å­˜ã•ã‚ŒãŸãƒ¬ãƒ¼ã‚¹IDæƒ…å ±ã‹ã‚‰ã€ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆã‚’å–å¾—
        
        Args:
            date_str: æ—¥ä»˜æ–‡å­—åˆ— (YYYYMMDD)
            
        Returns:
            List[str]: ãƒ¬ãƒ¼ã‚¹IDã®ãƒªã‚¹ãƒˆ
        """
        try:
            race_ids_file = get_race_ids_file_path(date_str)
            
            if not os.path.exists(race_ids_file):
                self.logger.warning(f"âš ï¸ ãƒ¬ãƒ¼ã‚¹IDæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {race_ids_file}")
                return []
            
            with open(race_ids_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            race_ids = []
            kaisai_data = data.get('kaisai_data', {})
            
            for venue, races in kaisai_data.items():
                for race in races:
                    race_id = race.get('race_id')
                    if race_id:
                        race_ids.append(race_id)
            
            self.logger.info(f"ğŸ“‹ ãƒ¬ãƒ¼ã‚¹IDå–å¾—å®Œäº†: {len(race_ids)}ä»¶")
            return race_ids
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ¬ãƒ¼ã‚¹IDå–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def close(self):
        """
        ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾
        """
        if hasattr(self.scraper, 'close'):
            self.scraper.close()
        self.logger.info("ğŸ”’ OptimizedDataFetcherã‚’é–‰ã˜ã¾ã—ãŸ") 