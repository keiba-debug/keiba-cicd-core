#!/usr/bin/env python3
"""
é«˜é€ŸãƒãƒƒãƒCLIï¼ˆrequestsç‰ˆï¼‰

RequestsScraperã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿç‰ˆã®ãƒãƒƒãƒå‡¦ç†CLI
- Seleniumä¸ä½¿ç”¨ã§å¤§å¹…ãªé«˜é€ŸåŒ–
- ä¸¦åˆ—å‡¦ç†å¯¾å¿œ
- è©³ç´°ãªé€²æ—è¡¨ç¤º
"""

import argparse
import logging
from datetime import datetime, timedelta

from .batch.optimized_data_fetcher import OptimizedDataFetcher
from .batch.core.common import parse_date, setup_batch_logger


def main():
    """
    é«˜é€ŸãƒãƒƒãƒCLIã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    """
    parser = argparse.ArgumentParser(
        description='ç«¶é¦¬ãƒ–ãƒƒã‚¯é«˜é€Ÿãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ ï¼ˆrequestsç‰ˆï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # é«˜é€Ÿãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹å–å¾—
  python -m src.keibabook.fast_batch_cli schedule --start-date 2025/06/07

  # é«˜é€Ÿãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
  python -m src.keibabook.fast_batch_cli data --start-date 2025/06/07 --max-workers 8

  # é«˜é€Ÿå…¨å‡¦ç†ï¼ˆæœ€å¤§æ€§èƒ½ï¼‰
  python -m src.keibabook.fast_batch_cli full --start-date 2025/06/07 --delay 0.5 --max-workers 10

ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„:
  - RequestsScraperã‚’ä½¿ç”¨ï¼ˆSeleniumä¸ä½¿ç”¨ï¼‰
  - ä¸¦åˆ—å‡¦ç†å¯¾å¿œï¼ˆæœ€å¤§22ã‚³ã‚¢æ´»ç”¨å¯èƒ½ï¼‰
  - 10-20å€ã®é€Ÿåº¦å‘ä¸Š
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰')
    
    # å…±é€šå¼•æ•°
    def add_common_args(subparser):
        subparser.add_argument('--start-date', required=True, help='é–‹å§‹æ—¥ (YYYY/MM/DD)')
        subparser.add_argument('--end-date', help='çµ‚äº†æ—¥ (YYYY/MM/DD, çœç•¥æ™‚ã¯é–‹å§‹æ—¥ã¨åŒã˜)')
        subparser.add_argument('--delay', type=float, default=1.0, help='ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.0ï¼‰')
        subparser.add_argument('--max-workers', type=int, default=5, help='ä¸¦åˆ—å‡¦ç†æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰')
        subparser.add_argument('--debug', action='store_true', help='ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰')
    
    # schedule ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰
    schedule_parser = subparsers.add_parser('schedule', help='ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹å–å¾—ï¼ˆé«˜é€Ÿç‰ˆï¼‰')
    add_common_args(schedule_parser)
    
    # data ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰
    data_parser = subparsers.add_parser('data', help='ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆé«˜é€Ÿä¸¦åˆ—ç‰ˆï¼‰')
    add_common_args(data_parser)
    data_parser.add_argument('--data-types', default='seiseki,shutsuba,cyokyo,danwa', 
                           help='ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: seiseki,shutsuba,cyokyo,danwaï¼‰')
    
    # full ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰
    full_parser = subparsers.add_parser('full', help='å…¨å‡¦ç†å®Ÿè¡Œï¼ˆè¶…é«˜é€Ÿç‰ˆï¼‰')
    add_common_args(full_parser)
    full_parser.add_argument('--data-types', default='seiseki,shutsuba,cyokyo,danwa',
                           help='ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: seiseki,shutsuba,cyokyo,danwaï¼‰')
    full_parser.add_argument('--wait-between-phases', type=float, default=2.0,
                           help='Phaseé–“å¾…æ©Ÿæ™‚é–“ï¼ˆç§’, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2.0ï¼‰')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # ãƒ­ã‚°è¨­å®š
    logger = setup_batch_logger('fast_batch_cli')
    logger.info("ğŸš€ é«˜é€ŸãƒãƒƒãƒCLIï¼ˆrequestsç‰ˆï¼‰ã‚’é–‹å§‹")
    logger.info(f"ğŸ“Š ã‚³ãƒãƒ³ãƒ‰: {args.command}")
    logger.info(f"âš¡ ä¸¦åˆ—å‡¦ç†æ•°: {args.max_workers}")
    logger.info(f"ğŸ”¥ RequestsScraperã‚’ä½¿ç”¨ï¼ˆSeleniumä¸ä½¿ç”¨ï¼‰")
    
    try:
        # æ—¥ä»˜è§£æ
        start_date = parse_date(args.start_date)
        end_date = parse_date(args.end_date) if args.end_date else start_date
        
        logger.info(f"ğŸ“… å‡¦ç†æœŸé–“: {start_date} ï½ {end_date}")
        
        # OptimizedDataFetcherã‚’åˆæœŸåŒ–
        fetcher = OptimizedDataFetcher(delay=args.delay, max_workers=args.max_workers)
        
        if args.command == 'schedule':
            # ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹å–å¾—ï¼ˆé«˜é€Ÿç‰ˆï¼‰
            logger.info("ğŸš€ é«˜é€Ÿãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹å–å¾—ã‚’é–‹å§‹")
            current_date = start_date
            total_success = 0
            total_failed = 0
            
            while current_date <= end_date:
                date_str = current_date.strftime("%Y%m%d")
                logger.info(f"âš¡ æ—¥ç¨‹å–å¾—: {date_str}")
                
                success = fetcher.fetch_race_schedule_fast(date_str)
                if success:
                    total_success += 1
                    logger.info(f"âœ… æ—¥ç¨‹å–å¾—æˆåŠŸ: {date_str}")
                else:
                    total_failed += 1
                    logger.error(f"âŒ æ—¥ç¨‹å–å¾—å¤±æ•—: {date_str}")
                
                current_date += timedelta(days=1)
            
            logger.info(f"ğŸ“Š æ—¥ç¨‹å–å¾—å®Œäº†: æˆåŠŸ {total_success}ä»¶, å¤±æ•— {total_failed}ä»¶")
            
        elif args.command == 'data':
            # ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆé«˜é€Ÿä¸¦åˆ—ç‰ˆï¼‰
            data_types = args.data_types.split(',')
            logger.info(f"ğŸš€ é«˜é€Ÿä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’é–‹å§‹: {', '.join(data_types)}")
            
            current_date = start_date
            total_success = 0
            total_failed = 0
            total_processing_time = 0
            
            while current_date <= end_date:
                date_str = current_date.strftime("%Y%m%d")
                logger.info(f"âš¡ ãƒ‡ãƒ¼ã‚¿å–å¾—: {date_str}")
                
                summary = fetcher.fetch_all_race_data_parallel_fast(date_str, data_types)
                if summary.get('success'):
                    total_success += summary.get('total_success', 0)
                    total_failed += summary.get('total_failed', 0)
                    total_processing_time += summary.get('processing_time_seconds', 0)
                    
                    logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {summary.get('total_success', 0)}ä»¶æˆåŠŸ, "
                              f"{summary.get('total_failed', 0)}ä»¶å¤±æ•—, "
                              f"{summary.get('processing_time_seconds', 0):.2f}ç§’")
                else:
                    logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {date_str}")
                
                current_date += timedelta(days=1)
            
            logger.info(f"ğŸ“Š å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")
            logger.info(f"âœ… ç·æˆåŠŸ: {total_success}ä»¶")
            logger.info(f"âŒ ç·å¤±æ•—: {total_failed}ä»¶")
            logger.info(f"â±ï¸ ç·å‡¦ç†æ™‚é–“: {total_processing_time:.2f}ç§’")
            if total_processing_time > 0:
                logger.info(f"ğŸš€ å¹³å‡å‡¦ç†é€Ÿåº¦: {(total_success + total_failed) / total_processing_time:.2f}ã‚¿ã‚¹ã‚¯/ç§’")
            
        elif args.command == 'full':
            # å…¨å‡¦ç†å®Ÿè¡Œï¼ˆè¶…é«˜é€Ÿç‰ˆï¼‰
            data_types = args.data_types.split(',')
            logger.info("ğŸš€ è¶…é«˜é€Ÿå…¨å‡¦ç†ã‚’é–‹å§‹ï¼ˆæ—¥ç¨‹å–å¾—â†’ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼‰")
            
            # Phase 1: ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹å–å¾—
            logger.info("âš¡ Phase 1: é«˜é€Ÿãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹å–å¾—")
            current_date = start_date
            schedule_success = 0
            schedule_failed = 0
            
            while current_date <= end_date:
                date_str = current_date.strftime("%Y%m%d")
                logger.info(f"ğŸ“… æ—¥ç¨‹å–å¾—: {date_str}")
                
                success = fetcher.fetch_race_schedule_fast(date_str)
                if success:
                    schedule_success += 1
                else:
                    schedule_failed += 1
                
                current_date += timedelta(days=1)
            
            logger.info(f"âœ… Phase 1å®Œäº†: æˆåŠŸ {schedule_success}ä»¶, å¤±æ•— {schedule_failed}ä»¶")
            
            # Phaseé–“å¾…æ©Ÿ
            import time
            logger.info(f"â¸ï¸ Phaseé–“å¾…æ©Ÿ: {args.wait_between_phases}ç§’")
            time.sleep(args.wait_between_phases)
            
            # Phase 2: ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—
            logger.info(f"âš¡ Phase 2: é«˜é€Ÿä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿å–å¾— ({', '.join(data_types)})")
            current_date = start_date
            total_success = 0
            total_failed = 0
            total_processing_time = 0
            
            while current_date <= end_date:
                date_str = current_date.strftime("%Y%m%d")
                logger.info(f"ğŸ‡ ãƒ‡ãƒ¼ã‚¿å–å¾—: {date_str}")
                
                summary = fetcher.fetch_all_race_data_parallel_fast(date_str, data_types)
                if summary.get('success'):
                    total_success += summary.get('total_success', 0)
                    total_failed += summary.get('total_failed', 0)
                    total_processing_time += summary.get('processing_time_seconds', 0)
                    
                    logger.info(f"âœ… {date_str}å®Œäº†: {summary.get('total_success', 0)}ä»¶æˆåŠŸ, "
                              f"{summary.get('total_failed', 0)}ä»¶å¤±æ•—, "
                              f"{summary.get('processing_time_seconds', 0):.2f}ç§’, "
                              f"{summary.get('tasks_per_second', 0):.2f}ã‚¿ã‚¹ã‚¯/ç§’")
                
                current_date += timedelta(days=1)
            
            # æœ€çµ‚ã‚µãƒãƒªãƒ¼
            logger.info("ğŸ‰ è¶…é«˜é€Ÿå…¨å‡¦ç†å®Œäº†")
            logger.info("=" * 60)
            logger.info("ğŸ“Š æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼")
            logger.info("=" * 60)
            logger.info(f"ğŸ“… Phase 1ï¼ˆæ—¥ç¨‹å–å¾—ï¼‰: æˆåŠŸ {schedule_success}ä»¶, å¤±æ•— {schedule_failed}ä»¶")
            logger.info(f"ğŸ‡ Phase 2ï¼ˆãƒ‡ãƒ¼ã‚¿å–å¾—ï¼‰: æˆåŠŸ {total_success}ä»¶, å¤±æ•— {total_failed}ä»¶")
            logger.info(f"â±ï¸ ç·å‡¦ç†æ™‚é–“: {total_processing_time:.2f}ç§’")
            if total_processing_time > 0:
                logger.info(f"ğŸš€ å¹³å‡å‡¦ç†é€Ÿåº¦: {(total_success + total_failed) / total_processing_time:.2f}ã‚¿ã‚¹ã‚¯/ç§’")
            logger.info("=" * 60)
        
        # ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾
        fetcher.close()
        logger.info("âœ… é«˜é€ŸãƒãƒƒãƒCLIå‡¦ç†å®Œäº†")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise


if __name__ == '__main__':
    main() 