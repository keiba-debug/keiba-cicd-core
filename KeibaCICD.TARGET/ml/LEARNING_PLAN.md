# ç«¶é¦¬äºˆæ¸¬AI æ§‹ç¯‰ã—ãªãŒã‚‰å­¦ã¶æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ©ãƒ³

**ç›®æ¨™**: å®Ÿéš›ã«å‹•ãã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œã‚ŠãªãŒã‚‰ã€æ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤ã‹ã‚‰å®Ÿè·µã¾ã§ã‚’ç¿’å¾—ã™ã‚‹

ã“ã®ãƒ—ãƒ©ãƒ³ã§ã¯ã€KeibaCICDã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ã€æ®µéšçš„ã«æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¦ã„ãã¾ã™ã€‚
å„ã‚¹ãƒ†ãƒƒãƒ—ã§**ç†è«–â†’å®Ÿè£…â†’æ¤œè¨¼**ã®ã‚µã‚¤ã‚¯ãƒ«ã‚’å›ã—ã€ç¢ºå®Ÿã«ç†è§£ã‚’æ·±ã‚ã¾ã™ã€‚

---

## ğŸ“š å‰æçŸ¥è­˜

### å¿…è¦ãªã‚¹ã‚­ãƒ«
- PythonåŸºç¤ï¼ˆé–¢æ•°ã€ã‚¯ãƒ©ã‚¹ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰
- pandasåŸºç¤ï¼ˆDataFrameæ“ä½œï¼‰
- ãƒ•ã‚¡ã‚¤ãƒ«å…¥å‡ºåŠ›

### ä¸è¦ãªã‚¹ã‚­ãƒ«ï¼ˆã“ã‚Œã‹ã‚‰å­¦ã¶ï¼‰
- æ©Ÿæ¢°å­¦ç¿’ç†è«–
- çµ±è¨ˆå­¦
- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è©³ç´°

---

## ğŸ¯ å­¦ç¿’ã®å…¨ä½“åƒ

```
Phase 0: ç’°å¢ƒæ§‹ç¯‰ã¨åŸºç¤ç†è§£ï¼ˆ1æ—¥ï¼‰
   â†“
Phase 1: ãƒ‡ãƒ¼ã‚¿ç†è§£ã¨å¯è¦–åŒ–ï¼ˆ2-3æ—¥ï¼‰
   â†“
Phase 2: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆ3-4æ—¥ï¼‰
   â†“
Phase 3: ã¯ã˜ã‚ã¦ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆ2-3æ—¥ï¼‰
   â†“
Phase 4: ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆ3-4æ—¥ï¼‰
   â†“
Phase 5: ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã¨é‹ç”¨ï¼ˆ2-3æ—¥ï¼‰
```

**åˆè¨ˆ**: ç´„2é€±é–“ã§åŸºç¤çš„ãªäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãŒå®Œæˆ

---

## Phase 0: ç’°å¢ƒæ§‹ç¯‰ã¨åŸºç¤ç†è§£ï¼ˆ1æ—¥ï¼‰

### ğŸ¯ ã“ã®Phaseã§å­¦ã¶ã“ã¨
- æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µï¼ˆæ•™å¸«ã‚ã‚Šå­¦ç¿’ã€åˆ†é¡å•é¡Œï¼‰
- å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å½¹å‰²
- é–‹ç™ºç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### ğŸ“¦ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```powershell
cd E:\share\KEIBA-CICD\_keiba\keiba-cicd-core\KeibaCICD.TARGET

# æ©Ÿæ¢°å­¦ç¿’ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
pip install pandas numpy scikit-learn lightgbm matplotlib seaborn jupyter

# æ—¢å­˜ã®common.jravanãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚‚ä½¿ã„ã¾ã™
```

### ğŸ’¡ æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã‚’ç†è§£ã™ã‚‹

#### 1. æ•™å¸«ã‚ã‚Šå­¦ç¿’ã¨ã¯ï¼Ÿ

```
[å…¥åŠ›ãƒ‡ãƒ¼ã‚¿]     â†’     [ãƒ¢ãƒ‡ãƒ«]     â†’     [äºˆæ¸¬çµæœ]
é¦¬ã®æƒ…å ±ã€èª¿æ•™                            ç€é †äºˆæ¸¬
ãƒ¬ãƒ¼ã‚¹æ¡ä»¶ãªã©                            (1ç€ã«ãªã‚‹ã‹ï¼Ÿ)

å­¦ç¿’æ™‚: éå»ã®ãƒ¬ãƒ¼ã‚¹çµæœï¼ˆç­”ãˆï¼‰ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
äºˆæ¸¬æ™‚: æœªæ¥ã®ãƒ¬ãƒ¼ã‚¹ã«å¯¾ã—ã¦äºˆæ¸¬ã‚’è¡Œã†
```

#### 2. åˆ†é¡å•é¡Œï¼ˆClassificationï¼‰

ç«¶é¦¬äºˆæ¸¬ã¯ã€Œ**2å€¤åˆ†é¡å•é¡Œ**ã€ã¨ã—ã¦æ‰±ã„ã¾ã™ï¼š
- **Positive (1)**: é¦¬åˆ¸åœå†…ï¼ˆ1-3ç€ï¼‰
- **Negative (0)**: é¦¬åˆ¸åœå¤–ï¼ˆ4ç€ä»¥ä¸‹ï¼‰

#### 3. ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«

| ãƒ¢ãƒ‡ãƒ« | ç‰¹å¾´ | å­¦ç¿’ç”¨é€” |
|--------|------|----------|
| ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° | ã‚·ãƒ³ãƒ—ãƒ«ã€è§£é‡ˆã—ã‚„ã™ã„ | åŸºç¤ã‚’å­¦ã¶ |
| LightGBM | é«˜ç²¾åº¦ã€å®Ÿç”¨çš„ | å®Ÿæˆ¦æŠ•å…¥ |

### ğŸ“ å®Ÿè·µèª²é¡Œ: Jupyter Notebookã§åŸºç¤ã‚’è©¦ã™

```powershell
# Jupyterèµ·å‹•
jupyter notebook
```

æ–°è¦ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ä½œæˆ: `notebooks/00_getting_started.ipynb`

```python
# ã‚»ãƒ«1: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns

print("âœ“ All libraries imported successfully!")

# ã‚»ãƒ«2: æ—¢å­˜ã®common.jravanãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’è©¦ã™
import sys
sys.path.insert(0, '..')

from common.jravan import (
    get_horse_id_by_name,
    analyze_horse_training,
    get_horse_info
)

# å®Ÿéš›ã«é¦¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ã¿ã‚‹
horse_name = "ãƒ‰ã‚¦ãƒ‡ãƒ¥ãƒ¼ã‚¹"
horse_info = get_horse_info(horse_name)
print(f"é¦¬å: {horse_info['name']}")
print(f"èª¿æ•™å¸«: {horse_info['trainer_name']}")

# èª¿æ•™ãƒ‡ãƒ¼ã‚¿ã‚‚å–å¾—
training = analyze_horse_training(horse_name, "20260125")
if training.get("final"):
    print(f"æœ€çµ‚è¿½åˆ‡: {training['final']['time_4f']:.1f}ç§’")
```

### âœ… Phase 0 ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ããŸ
- [ ] Jupyter NotebookãŒèµ·å‹•ã§ããŸ
- [ ] common.jravanã‹ã‚‰é¦¬ã®ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããŸ
- [ ] æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µï¼ˆæ•™å¸«ã‚ã‚Šå­¦ç¿’ã€åˆ†é¡å•é¡Œï¼‰ã‚’ç†è§£ã—ãŸ

---

## Phase 1: ãƒ‡ãƒ¼ã‚¿ç†è§£ã¨å¯è¦–åŒ–ï¼ˆ2-3æ—¥ï¼‰

### ğŸ¯ ã“ã®Phaseã§å­¦ã¶ã“ã¨
- ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ã¨æ„å‘³
- EDAï¼ˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼‰ã®åŸºæœ¬
- å¯è¦–åŒ–ã«ã‚ˆã‚‹æ´å¯Ÿã®ç™ºè¦‹
- ãƒ‡ãƒ¼ã‚¿å“è³ªã®ç¢ºèª

### ğŸ“Š Step 1-1: ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€

**ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯**: `notebooks/01_data_exploration.ipynb`

```python
import pandas as pd
from pathlib import Path

# ãƒ¬ãƒ¼ã‚¹çµæœãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
# ï¼ˆå®Ÿéš›ã®ãƒ‘ã‚¹ã¯ç’°å¢ƒã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
race_results_path = Path("../data/race_results.csv")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv(race_results_path)

# åŸºæœ¬æƒ…å ±ã®ç¢ºèª
print(f"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}")
print(f"ã‚«ãƒ©ãƒ æ•°: {len(df.columns)}")
print("\nã‚«ãƒ©ãƒ ä¸€è¦§:")
print(df.columns.tolist())

# æœ€åˆã®5è¡Œã‚’è¡¨ç¤º
df.head()
```

### ğŸ“ˆ Step 1-2: ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚’å¯è¦–åŒ–ã™ã‚‹

```python
import matplotlib.pyplot as plt
import seaborn as sns

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆWindowsï¼‰
plt.rcParams['font.sans-serif'] = ['MS Gothic']
plt.rcParams['axes.unicode_minus'] = False

# 1. ç€é †ã®åˆ†å¸ƒ
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='ç€é †')
plt.title('ç€é †ã®åˆ†å¸ƒ')
plt.xlabel('ç€é †')
plt.ylabel('é ­æ•°')
plt.show()

# 2. äººæ°—ã®åˆ†å¸ƒ
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='äººæ°—')
plt.title('äººæ°—ã®åˆ†å¸ƒ')
plt.xlabel('äººæ°—')
plt.ylabel('é ­æ•°')
plt.show()

# 3. äººæ°—ã¨ç€é †ã®é–¢ä¿‚
plt.figure(figsize=(12, 8))
sns.heatmap(pd.crosstab(df['äººæ°—'], df['ç€é †']), annot=True, fmt='d', cmap='YlOrRd')
plt.title('äººæ°— vs ç€é †ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆ')
plt.show()
```

**å­¦ã¶ãƒã‚¤ãƒ³ãƒˆ**:
- `countplot`: ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ä»¶æ•°ã‚’æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º
- `heatmap`: 2ã¤ã®ã‚«ãƒ†ã‚´ãƒªã®é–¢ä¿‚ã‚’è‰²ã§å¯è¦–åŒ–
- äººæ°—ã¨ç€é †ã®ç›¸é–¢ã‚’ç›®ã§ç¢ºèªã§ãã‚‹

### ğŸ” Step 1-3: ç›®çš„å¤‰æ•°ã‚’ä½œã‚‹

æ©Ÿæ¢°å­¦ç¿’ã§ã¯ã€Œ**ä½•ã‚’äºˆæ¸¬ã—ãŸã„ã‹**ã€ã‚’æ˜ç¢ºã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```python
# ç›®çš„å¤‰æ•°: é¦¬åˆ¸åœå†…ï¼ˆ1-3ç€ï¼‰ã‹ã©ã†ã‹
df['target'] = (df['ç€é †'] <= 3).astype(int)

# ç¢ºèª
print("ç›®çš„å¤‰æ•°ã®åˆ†å¸ƒ:")
print(df['target'].value_counts())

# å¯è¦–åŒ–
plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='target')
plt.title('ç›®çš„å¤‰æ•°ã®åˆ†å¸ƒï¼ˆ0=åœå¤–, 1=åœå†…ï¼‰')
plt.xticks([0, 1], ['åœå¤–(4ç€ä»¥ä¸‹)', 'åœå†…(1-3ç€)'])
plt.show()
```

**å­¦ã¶ãƒã‚¤ãƒ³ãƒˆ**:
- ç›®çš„å¤‰æ•°ï¼ˆtargetï¼‰ã¯äºˆæ¸¬ã—ãŸã„å€¤
- 2å€¤åˆ†é¡: 0 or 1 ã§è¡¨ç¾
- ã‚¯ãƒ©ã‚¹ã®ä¸å‡è¡¡ã‚’ç¢ºèªï¼ˆåœå†…ã¨åœå¤–ã®æ¯”ç‡ï¼‰

### ğŸ“Š Step 1-4: èª¿æ•™ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã™ã‚‹

```python
from common.jravan import analyze_horse_training

# ã‚µãƒ³ãƒ—ãƒ«: æœ€åˆã®100ãƒ¬ãƒ¼ã‚¹ã«èª¿æ•™ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
sample_df = df.head(100).copy()

# èª¿æ•™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹é–¢æ•°
def get_training_features(row):
    """ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‹ã‚‰èª¿æ•™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    try:
        training = analyze_horse_training(
            row['horse_id'],
            row['race_date'],
            days_back=14
        )

        if training.get('final'):
            return {
                'training_count': training.get('total_count', 0),
                'final_4f_time': training['final']['time_4f'],
                'has_good_time': int(training.get('has_good_time', False)),
                'n_sakamichi': training.get('n_sakamichi', 0),
            }
        else:
            return {
                'training_count': 0,
                'final_4f_time': 0,
                'has_good_time': 0,
                'n_sakamichi': 0,
            }
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        return {'training_count': 0, 'final_4f_time': 0, 'has_good_time': 0, 'n_sakamichi': 0}

# èª¿æ•™ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹ã®ã§æœ€åˆã¯å°‘æ•°ã§è©¦ã™ï¼‰
training_features = sample_df.apply(get_training_features, axis=1, result_type='expand')
sample_df = pd.concat([sample_df, training_features], axis=1)

# ç¢ºèª
print(sample_df[['horse_id', 'training_count', 'final_4f_time', 'has_good_time']].head())
```

**å­¦ã¶ãƒã‚¤ãƒ³ãƒˆ**:
- `apply`: DataFrameã®å„è¡Œã«é–¢æ•°ã‚’é©ç”¨
- æ—¢å­˜ã®common.jravanãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æ´»ç”¨
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®é‡è¦æ€§

### âœ… Phase 1 ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚ãŸ
- [ ] ç€é †ã€äººæ°—ã®åˆ†å¸ƒã‚’å¯è¦–åŒ–ã§ããŸ
- [ ] ç›®çš„å¤‰æ•°ï¼ˆtargetï¼‰ã‚’ä½œæˆã§ããŸ
- [ ] èª¿æ•™ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã§ããŸ
- [ ] ãƒ‡ãƒ¼ã‚¿ã®å‚¾å‘ã‚’ç†è§£ã§ããŸ

---

## Phase 2: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆ3-4æ—¥ï¼‰

### ğŸ¯ ã“ã®Phaseã§å­¦ã¶ã“ã¨
- ç‰¹å¾´é‡ï¼ˆFeatureï¼‰ã¨ã¯ä½•ã‹
- ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚’æ´»ã‹ã—ãŸç‰¹å¾´é‡è¨­è¨ˆ
- ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- æ•°å€¤ã®æ­£è¦åŒ–ãƒ»æ¨™æº–åŒ–

### ğŸ’¡ ç‰¹å¾´é‡ã¨ã¯ï¼Ÿ

æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã€‚ç«¶é¦¬äºˆæ¸¬ã§ã¯ï¼š

| ã‚«ãƒ©ãƒ å | ç‰¹å¾´é‡ã®ç¨®é¡ | ä¾‹ |
|----------|-------------|-----|
| æ€§åˆ¥ | ã‚«ãƒ†ã‚´ãƒª | ç‰¡ã€ç‰ |
| å¹´é½¢ | æ•°å€¤ | 3, 4, 5... |
| æ–¤é‡ | æ•°å€¤ | 54.0, 57.0... |
| è·é›¢ | æ•°å€¤ | 1200, 1600, 2000... |
| é¦¬å ´çŠ¶æ…‹ | ã‚«ãƒ†ã‚´ãƒª | è‰¯ã€ç¨é‡ã€é‡ã€ä¸è‰¯ |
| èª¿æ•™æœ¬æ•° | æ•°å€¤ | 8, 10, 12... |
| æœ€çµ‚è¿½åˆ‡ã‚¿ã‚¤ãƒ  | æ•°å€¤ | 51.2, 52.5... |

### ğŸ“ Step 2-1: åŸºæœ¬çš„ãªç‰¹å¾´é‡ã‚’æº–å‚™

**ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯**: `notebooks/02_feature_engineering.ipynb`

```python
import pandas as pd
import numpy as np

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆPhase 1ã§ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ï¼‰
df = pd.read_csv("../data/race_results_with_training.csv")

# 1. æ•°å€¤ç‰¹å¾´é‡
numerical_features = [
    'age',           # å¹´é½¢
    'weight',        # æ–¤é‡
    'distance',      # è·é›¢
    'popularity',    # äººæ°—
    'training_count', # èª¿æ•™æœ¬æ•°
    'final_4f_time',  # æœ€çµ‚è¿½åˆ‡4Fã‚¿ã‚¤ãƒ 
]

# 2. ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡
categorical_features = [
    'sex',           # æ€§åˆ¥
    'track_code',    # ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
    'track_condition', # é¦¬å ´çŠ¶æ…‹
    'race_class',    # ãƒ¬ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹
]

print(f"æ•°å€¤ç‰¹å¾´é‡: {len(numerical_features)}å€‹")
print(f"ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡: {len(categorical_features)}å€‹")
```

### ğŸ”¢ Step 2-2: ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¯æ•°å€¤ã—ã‹æ‰±ãˆãªã„ã®ã§ã€ã‚«ãƒ†ã‚´ãƒªã‚’æ•°å€¤ã«å¤‰æ›ã—ã¾ã™ã€‚

#### One-Hot Encoding

```python
from sklearn.preprocessing import OneHotEncoder

# æ€§åˆ¥ã‚’One-Hotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
# ä¾‹: ç‰¡ â†’ [1, 0, 0], ç‰ â†’ [0, 1, 0], ã‚» â†’ [0, 0, 1]

encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
sex_encoded = encoder.fit_transform(df[['sex']])

# DataFrameã«å¤‰æ›
sex_df = pd.DataFrame(
    sex_encoded,
    columns=[f'sex_{cat}' for cat in encoder.categories_[0]]
)

print(sex_df.head())
```

**å­¦ã¶ãƒã‚¤ãƒ³ãƒˆ**:
- One-Hot: ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«0/1ã®ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
- `sparse_output=False`: å¯†ãªé…åˆ—ã§è¿”ã™
- `handle_unknown='ignore'`: æœªçŸ¥ã®ã‚«ãƒ†ã‚´ãƒªã‚’ç„¡è¦–

#### Label Encodingï¼ˆé †åºãŒã‚ã‚‹å ´åˆï¼‰

```python
from sklearn.preprocessing import LabelEncoder

# é¦¬å ´çŠ¶æ…‹ï¼ˆè‰¯ < ç¨é‡ < é‡ < ä¸è‰¯ï¼‰
condition_map = {'è‰¯': 0, 'ç¨é‡': 1, 'é‡': 2, 'ä¸è‰¯': 3}
df['track_condition_encoded'] = df['track_condition'].map(condition_map)

print(df[['track_condition', 'track_condition_encoded']].head())
```

### ğŸ“Š Step 2-3: æ•°å€¤ã®æ¨™æº–åŒ–

æ•°å€¤ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒç•°ãªã‚‹ã¨å­¦ç¿’ãŒã†ã¾ãã„ã‹ãªã„ãŸã‚ã€æ¨™æº–åŒ–ã—ã¾ã™ã€‚

```python
from sklearn.preprocessing import StandardScaler

# æ¨™æº–åŒ–: å¹³å‡0ã€åˆ†æ•£1ã«å¤‰æ›
scaler = StandardScaler()

numerical_cols = ['age', 'weight', 'distance', 'training_count', 'final_4f_time']
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

print("æ¨™æº–åŒ–å¾Œã®çµ±è¨ˆé‡:")
print(df[numerical_cols].describe())
```

**å­¦ã¶ãƒã‚¤ãƒ³ãƒˆ**:
- StandardScaler: (x - å¹³å‡) / æ¨™æº–åå·®
- ã™ã¹ã¦ã®ç‰¹å¾´é‡ãŒåŒã˜ã‚¹ã‚±ãƒ¼ãƒ«ã«ãªã‚‹
- ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒå®‰å®šã™ã‚‹

### ğŸ¨ Step 2-4: ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚’æ´»ã‹ã—ãŸç‰¹å¾´é‡

ç«¶é¦¬ã®çŸ¥è­˜ã‚’ä½¿ã£ã¦æ–°ã—ã„ç‰¹å¾´é‡ã‚’ä½œã‚Šã¾ã™ã€‚

```python
# 1. è·é›¢å¤‰åŒ–é‡ï¼ˆå‰èµ°ã‹ã‚‰ã®è·é›¢å·®ï¼‰
df['distance_change'] = df.groupby('horse_id')['distance'].diff().fillna(0)

# 2. æ–¤é‡å¤‰åŒ–é‡
df['weight_change'] = df.groupby('horse_id')['weight'].diff().fillna(0)

# 3. ä¼‘ã¿æ˜ã‘ãƒ•ãƒ©ã‚°ï¼ˆå‰èµ°ã‹ã‚‰30æ—¥ä»¥ä¸Šï¼‰
df['race_date_dt'] = pd.to_datetime(df['race_date'])
df['days_since_last'] = df.groupby('horse_id')['race_date_dt'].diff().dt.days.fillna(0)
df['is_after_rest'] = (df['days_since_last'] >= 30).astype(int)

# 4. æ˜‡ç´šæˆ¦ãƒ•ãƒ©ã‚°ï¼ˆå‰èµ°ã‚ˆã‚Šã‚¯ãƒ©ã‚¹ãŒä¸Šï¼‰
class_order = {'æ–°é¦¬': 0, 'æœªå‹åˆ©': 1, '1å‹': 2, '2å‹': 3, '3å‹': 4, 'ã‚ªãƒ¼ãƒ—ãƒ³': 5, 'G3': 6, 'G2': 7, 'G1': 8}
df['class_code'] = df['race_class'].map(class_order)
df['prev_class'] = df.groupby('horse_id')['class_code'].shift(1).fillna(0)
df['is_class_up'] = (df['class_code'] > df['prev_class']).astype(int)

# 5. èª¿æ•™è©•ä¾¡ï¼ˆå¥½ã‚¿ã‚¤ãƒ ã‚ã‚ŠÃ—å‚è·¯æœ¬æ•°ï¼‰
df['training_score'] = df['has_good_time'] * df['n_sakamichi']

print("æ–°è¦ç‰¹å¾´é‡:")
print(df[['distance_change', 'weight_change', 'is_after_rest', 'is_class_up', 'training_score']].head(10))
```

**å­¦ã¶ãƒã‚¤ãƒ³ãƒˆ**:
- `groupby().diff()`: ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ã®å·®åˆ†
- `fillna(0)`: æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹
- ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ãŒé‡è¦ï¼ˆè·é›¢å¤‰åŒ–ã€æ˜‡ç´šæˆ¦ãªã©ï¼‰

### âœ… Phase 2 ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] ç‰¹å¾´é‡ã®æ¦‚å¿µã‚’ç†è§£ã§ããŸ
- [ ] ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ããŸ
- [ ] æ•°å€¤ã‚’æ¨™æº–åŒ–ã§ããŸ
- [ ] ç«¶é¦¬çŸ¥è­˜ã‚’æ´»ã‹ã—ãŸç‰¹å¾´é‡ã‚’ä½œæˆã§ããŸ
- [ ] æœ€çµ‚çš„ãªç‰¹å¾´é‡ã‚»ãƒƒãƒˆãŒæº–å‚™ã§ããŸ

---

## Phase 3: ã¯ã˜ã‚ã¦ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆ2-3æ—¥ï¼‰

### ğŸ¯ ã“ã®Phaseã§å­¦ã¶ã“ã¨
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
- ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ï¼ˆfitï¼‰
- äºˆæ¸¬ï¼ˆpredictï¼‰ã®å®Ÿè¡Œ
- ç²¾åº¦è©•ä¾¡ã®åŸºæœ¬

### ğŸ“š æ©Ÿæ¢°å­¦ç¿’ã®æµã‚Œ

```
1. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
   â””â”€ å­¦ç¿’ç”¨ï¼ˆéå»ãƒ‡ãƒ¼ã‚¿ï¼‰ã¨ãƒ†ã‚¹ãƒˆç”¨ï¼ˆæœªæ¥ãƒ‡ãƒ¼ã‚¿ï¼‰ã«åˆ†ã‘ã‚‹

2. ãƒ¢ãƒ‡ãƒ«è¨“ç·´
   â””â”€ å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ã‚‹

3. äºˆæ¸¬
   â””â”€ ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ã‚’è¡Œã†

4. è©•ä¾¡
   â””â”€ äºˆæ¸¬ãŒã©ã‚Œãã‚‰ã„å½“ãŸã£ãŸã‹ã‚’æ¸¬ã‚‹
```

### ğŸ“ Step 3-1: ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—ã§åˆ†å‰²

**é‡è¦**: ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ã¯æ™‚ç³»åˆ—ãªã®ã§ã€ãƒ©ãƒ³ãƒ€ãƒ ã«åˆ†å‰²ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚

```python
import pandas as pd
from datetime import datetime

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv("../data/race_results_featured.csv")

# æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
df['race_date_dt'] = pd.to_datetime(df['race_date'])
df = df.sort_values('race_date_dt')

# æ™‚ç³»åˆ—åˆ†å‰²: 2024å¹´12æœˆ31æ—¥ã¾ã§ã‚’å­¦ç¿’ã€2025å¹´ä»¥é™ã‚’ãƒ†ã‚¹ãƒˆ
split_date = '2024-12-31'

train_df = df[df['race_date'] <= split_date].copy()
test_df = df[df['race_date'] > split_date].copy()

print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(train_df)}ä»¶ ({train_df['race_date'].min()} ï½ {train_df['race_date'].max()})")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_df)}ä»¶ ({test_df['race_date'].min()} ï½ {test_df['race_date'].max()})")
```

**å­¦ã¶ãƒã‚¤ãƒ³ãƒˆ**:
- æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¯ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²NG
- æœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã§éå»ã‚’äºˆæ¸¬ã—ã¦ã¯ã„ã‘ãªã„ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ï¼‰
- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯å®Ÿæˆ¦ã‚’æƒ³å®š

### ğŸ¤– Step 3-2: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã§å­¦ç¿’

ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score

# ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’åˆ†é›¢
feature_cols = [
    'age', 'weight', 'distance', 'popularity',
    'training_count', 'final_4f_time', 'has_good_time',
    'distance_change', 'weight_change', 'is_after_rest',
    'is_class_up', 'training_score'
]

X_train = train_df[feature_cols].fillna(0)
y_train = train_df['target']

X_test = test_df[feature_cols].fillna(0)
y_test = test_df['target']

# ãƒ¢ãƒ‡ãƒ«ä½œæˆã¨å­¦ç¿’
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train, y_train)

print("âœ“ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†!")

# äºˆæ¸¬
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]  # ç¢ºç‡å€¤

# ç²¾åº¦è©•ä¾¡
accuracy = accuracy_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred_proba)

print(f"æ­£è§£ç‡: {accuracy:.3f}")
print(f"AUC: {auc:.3f}")
```

**å­¦ã¶ãƒã‚¤ãƒ³ãƒˆ**:
- `fit()`: ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
- `predict()`: 0 or 1 ã®äºˆæ¸¬
- `predict_proba()`: ç¢ºç‡å€¤ï¼ˆ0ï½1ï¼‰
- AUC: 0.5ãªã‚‰å½“ã¦ãšã£ã½ã†ã€1.0ãªã‚‰å®Œç’§

### ğŸ“Š Step 3-3: ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’ç¢ºèª

ã©ã®ç‰¹å¾´é‡ãŒåŠ¹ã„ã¦ã„ã‚‹ã‹ã‚’è¦‹ã¾ã™ã€‚

```python
import matplotlib.pyplot as plt

# ä¿‚æ•°ï¼ˆé‡è¦åº¦ï¼‰ã‚’å–å¾—
coefficients = pd.DataFrame({
    'feature': feature_cols,
    'coefficient': model.coef_[0]
})

# çµ¶å¯¾å€¤ã§ã‚½ãƒ¼ãƒˆ
coefficients['abs_coef'] = coefficients['coefficient'].abs()
coefficients = coefficients.sort_values('abs_coef', ascending=False)

# å¯è¦–åŒ–
plt.figure(figsize=(10, 8))
plt.barh(coefficients['feature'], coefficients['coefficient'])
plt.xlabel('ä¿‚æ•°')
plt.title('ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼‰')
plt.tight_layout()
plt.show()

print(coefficients)
```

**å­¦ã¶ãƒã‚¤ãƒ³ãƒˆ**:
- æ­£ã®ä¿‚æ•°: å€¤ãŒå¤§ãã„ã»ã©åœå†…ã«ãªã‚Šã‚„ã™ã„
- è² ã®ä¿‚æ•°: å€¤ãŒå¤§ãã„ã»ã©åœå¤–ã«ãªã‚Šã‚„ã™ã„
- é‡è¦åº¦ã‚’è¦‹ã¦ç‰¹å¾´é‡ã‚’æ”¹å–„

### ğŸš€ Step 3-4: LightGBMã§é«˜ç²¾åº¦åŒ–

ã‚ˆã‚Šå¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¾ã™ã€‚

```python
import lightgbm as lgb

# LightGBMç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
params = {
    'objective': 'binary',        # 2å€¤åˆ†é¡
    'metric': 'auc',              # AUCã§è©•ä¾¡
    'boosting': 'gbdt',           # å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.8,
    'verbose': -1
}

# å­¦ç¿’
gbm = lgb.train(
    params,
    train_data,
    num_boost_round=1000,
    valid_sets=[test_data],
    callbacks=[lgb.early_stopping(stopping_rounds=50)]
)

print("âœ“ LightGBMå­¦ç¿’å®Œäº†!")

# äºˆæ¸¬
y_pred_gbm_proba = gbm.predict(X_test)
y_pred_gbm = (y_pred_gbm_proba > 0.5).astype(int)

# è©•ä¾¡
accuracy_gbm = accuracy_score(y_test, y_pred_gbm)
auc_gbm = roc_auc_score(y_test, y_pred_gbm_proba)

print(f"LightGBM æ­£è§£ç‡: {accuracy_gbm:.3f}")
print(f"LightGBM AUC: {auc_gbm:.3f}")
```

**å­¦ã¶ãƒã‚¤ãƒ³ãƒˆ**:
- LightGBMã¯å¤šãã®æ±ºå®šæœ¨ã‚’çµ„ã¿åˆã‚ã›ã‚‹
- `early_stopping`: éå­¦ç¿’ã‚’é˜²ã
- ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚ˆã‚Šé«˜ç²¾åº¦ã«ãªã‚‹ã“ã¨ãŒå¤šã„

### âœ… Phase 3 ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—ã§åˆ†å‰²ã§ããŸ
- [ ] ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã§å­¦ç¿’ãƒ»äºˆæ¸¬ã§ããŸ
- [ ] AUCã®æ„å‘³ã‚’ç†è§£ã§ããŸ
- [ ] ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’ç¢ºèªã§ããŸ
- [ ] LightGBMã§é«˜ç²¾åº¦åŒ–ã§ããŸ

---

## Phase 4: ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆ3-4æ—¥ï¼‰

### ğŸ¯ ã“ã®Phaseã§å­¦ã¶ã“ã¨
- æ··åŒè¡Œåˆ—ï¼ˆConfusion Matrixï¼‰
- é©åˆç‡ï¼ˆPrecisionï¼‰ã¨å†ç¾ç‡ï¼ˆRecallï¼‰
- ã—ãã„å€¤ã®èª¿æ•´
- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### ğŸ“Š Step 4-1: æ··åŒè¡Œåˆ—ã§è©³ç´°åˆ†æ

```python
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# æ··åŒè¡Œåˆ—
cm = confusion_matrix(y_test, y_pred_gbm)

# å¯è¦–åŒ–
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('æ··åŒè¡Œåˆ—')
plt.xlabel('äºˆæ¸¬å€¤')
plt.ylabel('å®Ÿéš›ã®å€¤')
plt.xticks([0.5, 1.5], ['åœå¤–(0)', 'åœå†…(1)'])
plt.yticks([0.5, 1.5], ['åœå¤–(0)', 'åœå†…(1)'])
plt.show()

print(cm)
```

**æ··åŒè¡Œåˆ—ã®èª­ã¿æ–¹**:

```
                äºˆæ¸¬
              åœå¤–  åœå†…
å®Ÿéš› åœå¤–   [TN   FP]
     åœå†…   [FN   TP]

TN (True Negative):  åœå¤–ã‚’åœå¤–ã¨æ­£ã—ãäºˆæ¸¬
FP (False Positive): åœå¤–ã‚’åœå†…ã¨èª¤äºˆæ¸¬
FN (False Negative): åœå†…ã‚’åœå¤–ã¨èª¤äºˆæ¸¬ï¼ˆè¦‹é€ƒã—ï¼‰
TP (True Positive):  åœå†…ã‚’åœå†…ã¨æ­£ã—ãäºˆæ¸¬
```

### ğŸ“ˆ Step 4-2: é©åˆç‡ã¨å†ç¾ç‡

```python
from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_test, y_pred_gbm)
recall = recall_score(y_test, y_pred_gbm)
f1 = f1_score(y_test, y_pred_gbm)

print(f"é©åˆç‡ (Precision): {precision:.3f}")
print(f"å†ç¾ç‡ (Recall): {recall:.3f}")
print(f"F1ã‚¹ã‚³ã‚¢: {f1:.3f}")

# è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, y_pred_gbm, target_names=['åœå¤–', 'åœå†…']))
```

**æŒ‡æ¨™ã®æ„å‘³**:
- **é©åˆç‡**: åœå†…ã¨äºˆæ¸¬ã—ãŸã†ã¡ã€å®Ÿéš›ã«åœå†…ã ã£ãŸå‰²åˆï¼ˆçš„ä¸­ç‡ï¼‰
- **å†ç¾ç‡**: å®Ÿéš›ã®åœå†…ã®ã†ã¡ã€æ­£ã—ãäºˆæ¸¬ã§ããŸå‰²åˆï¼ˆç¶²ç¾…ç‡ï¼‰
- **F1ã‚¹ã‚³ã‚¢**: é©åˆç‡ã¨å†ç¾ç‡ã®èª¿å’Œå¹³å‡

### ğŸšï¸ Step 4-3: ã—ãã„å€¤ã®èª¿æ•´

é¦¬åˆ¸æˆ¦ç•¥ã«å¿œã˜ã¦ã—ãã„å€¤ã‚’å¤‰ãˆã¾ã™ã€‚

```python
# ã—ãã„å€¤ã‚’å¤‰ãˆã¦é©åˆç‡ãƒ»å†ç¾ç‡ã‚’è¨ˆç®—
thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]

results = []
for threshold in thresholds:
    y_pred_custom = (y_pred_gbm_proba > threshold).astype(int)
    precision = precision_score(y_test, y_pred_custom)
    recall = recall_score(y_test, y_pred_custom)

    results.append({
        'threshold': threshold,
        'precision': precision,
        'recall': recall,
        'n_predictions': y_pred_custom.sum()  # è²·ã„ç›®æ•°
    })

results_df = pd.DataFrame(results)
print(results_df)

# å¯è¦–åŒ–
fig, ax1 = plt.subplots(figsize=(10, 6))

ax1.plot(results_df['threshold'], results_df['precision'], 'b-', label='é©åˆç‡')
ax1.plot(results_df['threshold'], results_df['recall'], 'r-', label='å†ç¾ç‡')
ax1.set_xlabel('ã—ãã„å€¤')
ax1.set_ylabel('ã‚¹ã‚³ã‚¢')
ax1.legend(loc='upper left')

ax2 = ax1.twinx()
ax2.plot(results_df['threshold'], results_df['n_predictions'], 'g--', label='è²·ã„ç›®æ•°')
ax2.set_ylabel('è²·ã„ç›®æ•°', color='g')
ax2.legend(loc='upper right')

plt.title('ã—ãã„å€¤ã¨è©•ä¾¡æŒ‡æ¨™ã®é–¢ä¿‚')
plt.show()
```

**å­¦ã¶ãƒã‚¤ãƒ³ãƒˆ**:
- ã—ãã„å€¤ã‚’ä¸Šã’ã‚‹ â†’ çš„ä¸­ç‡â†‘ã€è²·ã„ç›®æ•°â†“ï¼ˆå …å®Ÿï¼‰
- ã—ãã„å€¤ã‚’ä¸‹ã’ã‚‹ â†’ çš„ä¸­ç‡â†“ã€è²·ã„ç›®æ•°â†‘ï¼ˆç©æ¥µçš„ï¼‰
- æˆ¦ç•¥ã«å¿œã˜ã¦èª¿æ•´

### âš™ï¸ Step 4-4: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

LightGBMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚

```python
from sklearn.model_selection import TimeSeriesSplit
import optuna

# Optunaç›®çš„é–¢æ•°
def objective(trial):
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting': 'gbdt',
        'num_leaves': trial.suggest_int('num_leaves', 20, 100),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'verbose': -1
    }

    # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    tscv = TimeSeriesSplit(n_splits=3)
    auc_scores = []

    for train_idx, valid_idx in tscv.split(X_train):
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]
        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]

        train_data = lgb.Dataset(X_tr, label=y_tr)
        valid_data = lgb.Dataset(X_val, label=y_val)

        model = lgb.train(params, train_data, num_boost_round=500, valid_sets=[valid_data],
                         callbacks=[lgb.early_stopping(stopping_rounds=30)])

        y_pred = model.predict(X_val)
        auc = roc_auc_score(y_val, y_pred)
        auc_scores.append(auc)

    return np.mean(auc_scores)

# æœ€é©åŒ–å®Ÿè¡Œ
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50, show_progress_bar=True)

print(f"ãƒ™ã‚¹ãƒˆAUC: {study.best_value:.4f}")
print("ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
print(study.best_params)
```

**å­¦ã¶ãƒã‚¤ãƒ³ãƒˆ**:
- Optunaã§è‡ªå‹•çš„ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢
- TimeSeriesSplit: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”¨ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
- AUCã‚’æœ€å¤§åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ã‚‹

### âœ… Phase 4 ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] æ··åŒè¡Œåˆ—ã‚’ä½œæˆãƒ»ç†è§£ã§ããŸ
- [ ] é©åˆç‡ãƒ»å†ç¾ç‡ã®æ„å‘³ã‚’ç†è§£ã§ããŸ
- [ ] ã—ãã„å€¤ã‚’èª¿æ•´ã§ããŸ
- [ ] ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã§ããŸ
- [ ] ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æœ€å¤§é™ã«å¼•ãå‡ºã›ãŸ

---

## Phase 5: ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã¨é‹ç”¨ï¼ˆ2-3æ—¥ï¼‰

### ğŸ¯ ã“ã®Phaseã§å­¦ã¶ã“ã¨
- ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã®å®Ÿè£…
- å›åç‡ã®è¨ˆç®—
- ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨èª­ã¿è¾¼ã¿
- å®Ÿé‹ç”¨ã¸ã®å±•é–‹

### ğŸ’° Step 5-1: å›åç‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

```python
import pandas as pd

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«äºˆæ¸¬ç¢ºç‡ã‚’è¿½åŠ 
test_df_eval = test_df.copy()
test_df_eval['pred_proba'] = y_pred_gbm_proba
test_df_eval['pred_class'] = y_pred_gbm

# ã—ãã„å€¤0.6ã§è²·ã„ç›®ã‚’é¸æŠ
threshold = 0.6
bet_df = test_df_eval[test_df_eval['pred_proba'] >= threshold].copy()

print(f"è²·ã„ç›®æ•°: {len(bet_df)}")

# çš„ä¸­æ•°ã¨çš„ä¸­ç‡
hit_count = (bet_df['target'] == 1).sum()
hit_rate = hit_count / len(bet_df) if len(bet_df) > 0 else 0

print(f"çš„ä¸­æ•°: {hit_count}")
print(f"çš„ä¸­ç‡: {hit_rate:.1%}")

# å›åç‡è¨ˆç®—ï¼ˆä»®ã«ã‚ªãƒƒã‚ºæƒ…å ±ãŒã‚ã‚‹å ´åˆï¼‰
# ã“ã“ã§ã¯å˜å‹ã‚ªãƒƒã‚ºãŒã‚ã‚‹ã¨ä»®å®š
if 'win_odds' in bet_df.columns:
    bet_df['return'] = bet_df.apply(
        lambda row: row['win_odds'] * 100 if row['target'] == 1 else 0,
        axis=1
    )

    total_bet = len(bet_df) * 100  # 1ç‚¹100å††
    total_return = bet_df['return'].sum()
    recovery_rate = (total_return / total_bet) * 100

    print(f"\næŠ•è³‡é¡: {total_bet:,}å††")
    print(f"æ‰•æˆ»é¡: {total_return:,.0f}å††")
    print(f"åæ”¯: {total_return - total_bet:+,.0f}å††")
    print(f"å›åç‡: {recovery_rate:.1f}%")
```

### ğŸ“Š Step 5-2: æœˆæ¬¡åæ”¯ã®å¯è¦–åŒ–

```python
import matplotlib.pyplot as plt

# æœˆã”ã¨ã«é›†è¨ˆ
bet_df['race_month'] = pd.to_datetime(bet_df['race_date']).dt.to_period('M')

monthly_stats = bet_df.groupby('race_month').apply(
    lambda g: pd.Series({
        'bet_count': len(g),
        'hit_count': (g['target'] == 1).sum(),
        'total_return': g['return'].sum() if 'return' in g.columns else 0,
        'total_bet': len(g) * 100
    })
).reset_index()

monthly_stats['recovery_rate'] = (monthly_stats['total_return'] / monthly_stats['total_bet']) * 100

# å¯è¦–åŒ–
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

# å›åç‡ã®æ¨ç§»
ax1.bar(range(len(monthly_stats)), monthly_stats['recovery_rate'], alpha=0.7)
ax1.axhline(y=100, color='r', linestyle='--', label='æç›Šåˆ†å²ç‚¹')
ax1.set_xlabel('æœˆ')
ax1.set_ylabel('å›åç‡ (%)')
ax1.set_title('æœˆæ¬¡å›åç‡ã®æ¨ç§»')
ax1.legend()
ax1.grid(True, alpha=0.3)

# è²·ã„ç›®æ•°ã¨çš„ä¸­æ•°
ax2.bar(range(len(monthly_stats)), monthly_stats['bet_count'], alpha=0.5, label='è²·ã„ç›®æ•°')
ax2.bar(range(len(monthly_stats)), monthly_stats['hit_count'], alpha=0.7, label='çš„ä¸­æ•°')
ax2.set_xlabel('æœˆ')
ax2.set_ylabel('ä»¶æ•°')
ax2.set_title('æœˆæ¬¡è²·ã„ç›®æ•°ã¨çš„ä¸­æ•°')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(monthly_stats)
```

### ğŸ’¾ Step 5-3: ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜

```python
import joblib

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
model_path = "../data/models/lightgbm_model.pkl"
joblib.dump(gbm, model_path)
print(f"âœ“ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_path}")

# ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚‚ä¿å­˜
scaler_path = "../data/models/scaler.pkl"
joblib.dump(scaler, scaler_path)
print(f"âœ“ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜å®Œäº†: {scaler_path}")

# ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚‚ä¿å­˜
import json
feature_info = {
    'features': feature_cols,
    'threshold': 0.6,
    'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
}

with open("../data/models/model_info.json", "w", encoding="utf-8") as f:
    json.dump(feature_info, f, ensure_ascii=False, indent=2)

print("âœ“ ãƒ¢ãƒ‡ãƒ«æƒ…å ±ä¿å­˜å®Œäº†")
```

### ğŸš€ Step 5-4: æ–°ã—ã„ãƒ¬ãƒ¼ã‚¹ã®äºˆæ¸¬

```python
def predict_new_race(race_data):
    """
    æ–°ã—ã„ãƒ¬ãƒ¼ã‚¹ã®äºˆæ¸¬ã‚’è¡Œã†

    Args:
        race_data: äºˆæ¸¬å¯¾è±¡ã®DataFrame

    Returns:
        äºˆæ¸¬çµæœã‚’å«ã‚€DataFrame
    """
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model = joblib.load("../data/models/lightgbm_model.pkl")
    scaler = joblib.load("../data/models/scaler.pkl")

    with open("../data/models/model_info.json", "r", encoding="utf-8") as f:
        model_info = json.load(f)

    feature_cols = model_info['features']
    threshold = model_info['threshold']

    # ç‰¹å¾´é‡æº–å‚™
    X_new = race_data[feature_cols].fillna(0)

    # äºˆæ¸¬
    pred_proba = model.predict(X_new)
    pred_class = (pred_proba >= threshold).astype(int)

    # çµæœã‚’è¿½åŠ 
    result = race_data.copy()
    result['pred_proba'] = pred_proba
    result['pred_class'] = pred_class
    result['recommended'] = (pred_proba >= threshold)

    # æ¨å¥¨è²·ã„ç›®ã®ã¿è¿”ã™
    return result[result['recommended']].sort_values('pred_proba', ascending=False)

# ä½¿ç”¨ä¾‹
# new_race_df = pd.read_csv("../data/upcoming_races.csv")
# recommendations = predict_new_race(new_race_df)
# print(recommendations[['horse_name', 'pred_proba', 'umaban']])
```

### ğŸ“ Step 5-5: äºˆæ¸¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

```python
def generate_prediction_report(predictions, race_info):
    """
    äºˆæ¸¬çµæœã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    """
    report = f"""
# ç«¶é¦¬äºˆæ¸¬ãƒ¬ãƒãƒ¼ãƒˆ

## ãƒ¬ãƒ¼ã‚¹æƒ…å ±
- æ—¥ä»˜: {race_info['date']}
- ç«¶é¦¬å ´: {race_info['track']}
- ãƒ¬ãƒ¼ã‚¹ç•ªå·: {race_info['race_num']}R
- è·é›¢: {race_info['distance']}m

## æ¨å¥¨è²·ã„ç›®ï¼ˆ{len(predictions)}ç‚¹ï¼‰

"""

    for idx, row in predictions.iterrows():
        report += f"""
### {row['umaban']}ç•ª {row['horse_name']}
- **äºˆæ¸¬ç¢ºç‡**: {row['pred_proba']:.1%}
- æ€§é½¢: {row['sex']}{row['age']}æ­³
- æ–¤é‡: {row['weight']}kg
- èª¿æ•™è©•ä¾¡: {row['training_score']:.1f}
- æœ€çµ‚è¿½åˆ‡: {row['final_4f_time']:.1f}ç§’

"""

    return report

# ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ä¾‹
# race_info = {'date': '2026-02-01', 'track': 'æ±äº¬', 'race_num': 11, 'distance': 2000}
# report = generate_prediction_report(recommendations, race_info)
# print(report)
```

### âœ… Phase 5 ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè£…ã§ããŸ
- [ ] å›åç‡ã‚’è¨ˆç®—ã§ããŸ
- [ ] æœˆæ¬¡åæ”¯ã‚’å¯è¦–åŒ–ã§ããŸ
- [ ] ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ã§ããŸ
- [ ] æ–°ã—ã„ãƒ¬ãƒ¼ã‚¹ã®äºˆæ¸¬ãŒã§ããŸ
- [ ] äºˆæ¸¬ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã§ããŸ

---

## ğŸ“ å’æ¥­èª²é¡Œ: ç·åˆæ¼”ç¿’

ã™ã¹ã¦ã®Phaseã‚’çµ„ã¿åˆã‚ã›ã¦ã€å®Ÿæˆ¦çš„ãªã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

### èª²é¡Œ1: é€±æœ«ãƒ¬ãƒ¼ã‚¹è‡ªå‹•äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 

```python
"""
é€±æœ«ã®ãƒ¬ãƒ¼ã‚¹ã‚’è‡ªå‹•äºˆæ¸¬ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ¯é€±é‡‘æ›œæ—¥ã«å®Ÿè¡Œ
"""

from common.jravan import build_race_id, get_horse_info, analyze_horse_training
import pandas as pd
from datetime import datetime, timedelta

def predict_weekend_races():
    """é€±æœ«ãƒ¬ãƒ¼ã‚¹ã‚’äºˆæ¸¬"""

    # 1. ä»Šé€±æœ«ã®æ—¥ä»˜ã‚’å–å¾—
    today = datetime.now()
    saturday = today + timedelta(days=(5 - today.weekday()))
    sunday = saturday + timedelta(days=1)

    # 2. ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå®Ÿè£…ã¯ç’°å¢ƒã«å¿œã˜ã¦ï¼‰
    races = get_weekend_races([saturday, sunday])

    # 3. å„ãƒ¬ãƒ¼ã‚¹ã®å‡ºèµ°é¦¬ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    all_predictions = []

    for race in races:
        race_id = race['race_id']
        horses = race['horses']

        # ç‰¹å¾´é‡ä½œæˆ
        race_df = prepare_race_features(horses, race)

        # äºˆæ¸¬
        predictions = predict_new_race(race_df)
        predictions['race_id'] = race_id

        all_predictions.append(predictions)

    # 4. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = generate_weekend_report(all_predictions)

    # 5. ä¿å­˜
    output_file = f"predictions_{saturday.strftime('%Y%m%d')}.md"
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"âœ“ äºˆæ¸¬å®Œäº†: {output_file}")

# å®Ÿè¡Œ
if __name__ == "__main__":
    predict_weekend_races()
```

### èª²é¡Œ2: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

```python
"""
ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ç¶™ç¶šçš„ã«ç›£è¦–
é€±æ¬¡ã§å®Ÿè¡Œã—ã€æ€§èƒ½åŠ£åŒ–ã‚’æ¤œçŸ¥
"""

def monitor_model_performance():
    """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’ç›£è¦–"""

    # æœ€æ–°1ãƒ¶æœˆã®ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—
    recent_results = get_recent_race_results(days=30)

    # äºˆæ¸¬ã‚’å®Ÿè¡Œ
    predictions = predict_new_race(recent_results)

    # å®Ÿéš›ã®çµæœã¨æ¯”è¼ƒ
    actual = recent_results['target']
    predicted = predictions['pred_class']

    # è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—
    from sklearn.metrics import accuracy_score, roc_auc_score

    accuracy = accuracy_score(actual, predicted)
    auc = roc_auc_score(actual, predictions['pred_proba'])

    # åŸºæº–å€¤ã¨æ¯”è¼ƒ
    baseline_accuracy = 0.65
    baseline_auc = 0.70

    if accuracy < baseline_accuracy or auc < baseline_auc:
        print("âš ï¸ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãŒä½ä¸‹ã—ã¦ã„ã¾ã™ã€‚å†å­¦ç¿’ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        print(f"ç¾åœ¨ã®æ­£è§£ç‡: {accuracy:.3f} (åŸºæº–: {baseline_accuracy})")
        print(f"ç¾åœ¨ã®AUC: {auc:.3f} (åŸºæº–: {baseline_auc})")
    else:
        print("âœ“ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¯æ­£å¸¸ã§ã™ã€‚")

    # ãƒ­ã‚°ä¿å­˜
    log_performance(accuracy, auc)
```

### èª²é¡Œ3: ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
"""
å®šæœŸçš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æœˆ1å›å®Ÿè¡Œ
"""

def retrain_model():
    """ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’"""

    print("=== ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’é–‹å§‹ ===")

    # 1. æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    print("1. ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
    df = load_latest_race_data()

    # 2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    print("2. ç‰¹å¾´é‡ä½œæˆä¸­...")
    df_featured = create_all_features(df)

    # 3. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    print("3. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ä¸­...")
    train_df, test_df = split_data_by_date(df_featured, split_date='2025-12-31')

    # 4. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    print("4. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
    model = train_lightgbm_model(train_df, test_df)

    # 5. è©•ä¾¡
    print("5. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­...")
    metrics = evaluate_model(model, test_df)

    # 6. å‰å›ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒ
    print("6. æ€§èƒ½æ¯”è¼ƒä¸­...")
    previous_metrics = load_previous_metrics()

    if metrics['auc'] > previous_metrics['auc']:
        print(f"âœ“ æ€§èƒ½å‘ä¸Š: AUC {previous_metrics['auc']:.4f} â†’ {metrics['auc']:.4f}")

        # 7. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        print("7. ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
        save_model(model, metrics)
        print("âœ“ æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
    else:
        print(f"âš ï¸ æ€§èƒ½ä½ä¸‹: AUC {previous_metrics['auc']:.4f} â†’ {metrics['auc']:.4f}")
        print("å¤ã„ãƒ¢ãƒ‡ãƒ«ã‚’ç¶­æŒã—ã¾ã™ã€‚")

    print("=== å†å­¦ç¿’å®Œäº† ===")
```

---

## ğŸ“š ã•ã‚‰ã«å­¦ã¶ãŸã‚ã«

### æ¨å¥¨ãƒªã‚½ãƒ¼ã‚¹

#### æ›¸ç±
1. **ã€ŒPythonã§ã¯ã˜ã‚ã‚‹æ©Ÿæ¢°å­¦ç¿’ã€** - scikit-learnã®åŸºç¤
2. **ã€ŒKaggleã§å‹ã¤ãƒ‡ãƒ¼ã‚¿åˆ†æã®æŠ€è¡“ã€** - å®Ÿè·µçš„ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
3. **ã€Œå‰å‡¦ç†å¤§å…¨ã€** - ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ã‚¹
1. **Coursera: Machine Learning** (Andrew Ng) - æ©Ÿæ¢°å­¦ç¿’ã®ç†è«–
2. **Kaggle Learn** - ç„¡æ–™ã®å®Ÿè·µãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«

#### ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£
1. **Kaggle** - ç«¶é¦¬äºˆæ¸¬ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³
2. **GitHub** - ç«¶é¦¬äºˆæ¸¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®äº‹ä¾‹

### ç™ºå±•çš„ãªãƒˆãƒ”ãƒƒã‚¯

#### 1. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’
è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦ç²¾åº¦å‘ä¸Š

```python
from sklearn.ensemble import VotingClassifier

ensemble = VotingClassifier(
    estimators=[
        ('lr', LogisticRegression()),
        ('lgbm', lgb.LGBMClassifier()),
        ('xgb', xgb.XGBClassifier())
    ],
    voting='soft'
)
```

#### 2. ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’

```python
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])
```

#### 3. å¼·åŒ–å­¦ç¿’
æœ€é©ãªè³­ã‘æˆ¦ç•¥ã‚’å­¦ç¿’

```python
# Q-learning for betting strategy
# çŠ¶æ…‹: ãƒ¬ãƒ¼ã‚¹çŠ¶æ³ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: è³­ã‘é‡‘é¡
```

#### 4. è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰
å©èˆã‚³ãƒ¡ãƒ³ãƒˆã‚„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±æŠ½å‡º

```python
from transformers import pipeline

sentiment = pipeline("sentiment-analysis", model="bert-base-japanese")
comment = "å¥½èª¿ã‚’ç¶­æŒã—ã¦ã„ã‚‹"
result = sentiment(comment)
```

---

## ğŸ ã¾ã¨ã‚

### å­¦ã‚“ã ã“ã¨

| Phase | å­¦ç¿’å†…å®¹ | å®Ÿè£…ã—ãŸã‚‚ã® |
|-------|----------|--------------|
| 0 | ç’°å¢ƒæ§‹ç¯‰ã€MLåŸºç¤ | Jupyterç’°å¢ƒã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒª |
| 1 | ãƒ‡ãƒ¼ã‚¿ç†è§£ã€å¯è¦–åŒ– | EDAã€ç›®çš„å¤‰æ•°ä½œæˆ |
| 2 | ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° | ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€æ¨™æº–åŒ–ã€ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ |
| 3 | ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã€äºˆæ¸¬ | ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã€LightGBM |
| 4 | è©•ä¾¡ã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | æ··åŒè¡Œåˆ—ã€ã—ãã„å€¤èª¿æ•´ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– |
| 5 | ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã€é‹ç”¨ | å›åç‡è¨ˆç®—ã€ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã€äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  |

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **ç²¾åº¦å‘ä¸Š**
   - æ–°ã—ã„ç‰¹å¾´é‡ã®è¿½åŠ ï¼ˆè¡€çµ±ã€ã‚³ãƒ¼ã‚¹é©æ€§ãªã©ï¼‰
   - ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã®å°å…¥
   - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å†æœ€é©åŒ–

2. **ã‚·ã‚¹ãƒ†ãƒ åŒ–**
   - è‡ªå‹•ãƒ‡ãƒ¼ã‚¿æ›´æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
   - Webãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆStreamlit/Dashï¼‰
   - LINE/Slacké€šçŸ¥æ©Ÿèƒ½

3. **å®Ÿé‹ç”¨**
   - å°‘é¡ã‹ã‚‰å®Ÿæˆ¦æŠ•å…¥
   - åæ”¯è¨˜éŒ²ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
   - ãƒ¢ãƒ‡ãƒ«ã®å®šæœŸçš„ãªå†å­¦ç¿’

### ç¶™ç¶šçš„ãªæ”¹å–„

```
é€±æ¬¡ã‚µã‚¤ã‚¯ãƒ«:
  é‡‘æ›œ: é€±æœ«ãƒ¬ãƒ¼ã‚¹äºˆæ¸¬
  åœŸæ—¥: çµæœç¢ºèªã€å®Ÿç¸¾è¨˜éŒ²
  æœˆæ›œ: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ

æœˆæ¬¡ã‚µã‚¤ã‚¯ãƒ«:
  æœˆåˆ: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ¬ãƒ“ãƒ¥ãƒ¼
  æœˆä¸­: ç‰¹å¾´é‡æ”¹å–„æ¤œè¨
  æœˆæœ«: ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’
```

---

**ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼** ğŸ‰

ã‚ãªãŸã¯ä»Šã€æ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤ã‹ã‚‰å®Ÿè·µçš„ãªç«¶é¦¬äºˆæ¸¬AIã¾ã§ã‚’ç¿’å¾—ã—ã¾ã—ãŸã€‚
ã“ã“ã‹ã‚‰ã¯å®Ÿéš›ã«é‹ç”¨ã—ãªãŒã‚‰ã€ç¶™ç¶šçš„ã«æ”¹å–„ã—ã¦ã„ãã¾ã—ã‚‡ã†ã€‚

---

*ä½œæˆæ—¥: 2026-01-30*
*å¯¾è±¡: KeibaCICD ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ*
