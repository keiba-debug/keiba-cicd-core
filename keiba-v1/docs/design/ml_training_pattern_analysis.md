# èª¿æ•™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ æ©Ÿæ¢°å­¦ç¿’è¨­è¨ˆæ›¸

## ğŸ“‹ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±

| é …ç›® | å†…å®¹ |
|------|------|
| **ä½œæˆæ—¥** | 2025å¹´12æœˆ14æ—¥ |
| **ãƒãƒ¼ã‚¸ãƒ§ãƒ³** | 1.0 |
| **ç›®çš„** | ã‚³ãƒ¼ã‚¹åˆ¥ãƒ»èª¿æ•™å¸«åˆ¥ã®æœ‰åŠ¹ãªèª¿æ•™ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹ã™ã‚‹ |

---

## 1. ç›®æ¨™ã¨æœŸå¾…ã•ã‚Œã‚‹æˆæœ

### 1.1 åˆ†æç›®æ¨™

| ç›®æ¨™ | èª¬æ˜ | æœŸå¾…ã•ã‚Œã‚‹æˆæœ |
|------|------|---------------|
| **ã‚³ãƒ¼ã‚¹åˆ¥æœ‰åŠ¹ãƒ‘ã‚¿ãƒ¼ãƒ³** | æ±äº¬èŠ1600mãªã©ç‰¹å®šæ¡ä»¶ã§å¥½èµ°ã™ã‚‹èª¿æ•™ãƒ‘ã‚¿ãƒ¼ãƒ³ | ã‚³ãƒ¼ã‚¹é©æ€§ã«åŸºã¥ãèª¿æ•™è©•ä¾¡ |
| **èª¿æ•™å¸«åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³** | å„èª¿æ•™å¸«ã®å¾—æ„ãªä»•ä¸Šã’æ–¹ã¨çµæœã®ç›¸é–¢ | èª¿æ•™å¸«åˆ¥ã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ |
| **æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³** | ãƒ¬ãƒ¼ã‚¹ç›´å‰ã®èª¿æ•™å¼·åº¦ã¨çµæœã®é–¢ä¿‚ | æœ€é©ãªèª¿æ•™é–“éš”ãƒ»å¼·åº¦ã®ç™ºè¦‹ |

### 1.2 ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤

```
å…¥åŠ›: èª¿æ•™ãƒ‡ãƒ¼ã‚¿ + ãƒ¬ãƒ¼ã‚¹æ¡ä»¶ + èª¿æ•™å¸«æƒ…å ±
  â†“
åˆ†æ: æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«
  â†“
å‡ºåŠ›: ã€Œã“ã®èª¿æ•™ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯æ±äº¬èŠ1600mã§é«˜ã„æˆåŠŸç‡ã€
      ã€Œã“ã®èª¿æ•™å¸«ã®ã“ã®ä»•ä¸Šã’æ–¹ã¯â—ã€
```

---

## 2. åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿

### 2.1 ç«¶é¦¬ãƒ–ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ï¼ˆæ—¢å­˜ï¼‰

| ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ | ä¸»è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ | ç”¨é€” |
|-------------|---------------|------|
| **CyokyoParser** | attack_explanation, short_review, training_arrow | èª¿æ•™è©•ä¾¡ |
| **SeisekiParser** | ç€é †, ã‚¿ã‚¤ãƒ , é€šéé †ä½, ä¸ŠãŒã‚Š | ç›®çš„å¤‰æ•°ï¼ˆæˆåŠŸ/å¤±æ•—ï¼‰ |
| **SyutubaParser** | ã‚³ãƒ¼ã‚¹, è·é›¢, é¦¬å ´çŠ¶æ…‹ | ãƒ¬ãƒ¼ã‚¹æ¡ä»¶ |
| **NitteiParser** | é–‹å‚¬å ´æ‰€, ãƒ¬ãƒ¼ã‚¹ID | ã‚³ãƒ¼ã‚¹åˆ†é¡ |

### 2.2 JRA-VANãƒ‡ãƒ¼ã‚¿ï¼ˆå°†æ¥çµ±åˆäºˆå®šï¼‰

| ãƒ‡ãƒ¼ã‚¿ | å†…å®¹ | è¿½åŠ ä¾¡å€¤ |
|--------|------|---------|
| èª¿æ•™ã‚¿ã‚¤ãƒ  | å‚è·¯/CW/ãƒãƒªãƒˆãƒ©ãƒƒã‚¯ç­‰ã®ã‚¿ã‚¤ãƒ  | å®šé‡çš„è©•ä¾¡ |
| èª¿æ•™å¸«ID | èª¿æ•™å¸«ãƒã‚¹ã‚¿ | èª¿æ•™å¸«åˆ¥é›†è¨ˆ |
| é¦¬å ´çŠ¶æ…‹ | è‰¯/ç¨é‡/é‡/ä¸è‰¯ | æ¡ä»¶åˆ†å² |
| éå»æˆç¸¾ | å…¨ãƒ¬ãƒ¼ã‚¹å±¥æ­´ | é¦¬ã®èƒ½åŠ›è©•ä¾¡ |

### 2.3 ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¤ãƒ¡ãƒ¼ã‚¸

```sql
-- åˆ†æç”¨çµ±åˆãƒ“ãƒ¥ãƒ¼ä¾‹
CREATE VIEW ml.TrainingAnalysis AS
SELECT 
    r.race_id,
    r.course_type,        -- èŠ/ãƒ€ãƒ¼ãƒˆ
    r.distance,           -- è·é›¢
    r.venue,              -- é–‹å‚¬å ´æ‰€
    t.trainer_id,         -- èª¿æ•™å¸«ID
    t.trainer_name,       -- èª¿æ•™å¸«å
    cy.training_arrow,    -- èª¿æ•™çŸ¢å°ï¼ˆâ†‘â†—â†’â†˜â†“ï¼‰
    cy.short_review,      -- çŸ­è©•ï¼ˆå¥½ä»•ä¸ŠãŒã‚Šç­‰ï¼‰
    cy.attack_explanation,-- æ”»ã‚è§£èª¬ãƒ†ã‚­ã‚¹ãƒˆ
    s.finish_position,    -- ç€é †ï¼ˆç›®çš„å¤‰æ•°ï¼‰
    s.time,               -- èµ°ç ´ã‚¿ã‚¤ãƒ 
    s.last_3f             -- ä¸ŠãŒã‚Š3F
FROM keibabook.Races r
JOIN keibabook.TrainingData cy ON r.race_id = cy.race_id
JOIN keibabook.Results s ON r.race_id = s.race_id AND cy.horse_number = s.horse_number
LEFT JOIN jravan.Trainers t ON ...;
```

---

## 3. æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

### 3.1 å•é¡Œè¨­å®š

| ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ | å•é¡Œã‚¿ã‚¤ãƒ— | ç›®çš„å¤‰æ•° | é©ç”¨å ´é¢ |
|-----------|-----------|---------|---------|
| **åˆ†é¡** | Binary/Multi-class | 3ç€ä»¥å†…ï¼ˆ1/0ï¼‰ã€ç€é †åŒºåˆ† | å‹ç‡äºˆæ¸¬ |
| **å›å¸°** | é€£ç¶šå€¤äºˆæ¸¬ | ç€é †ã€ã‚¿ã‚¤ãƒ å·® | ç´°ã‹ã„é †ä½äºˆæ¸¬ |
| **ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°** | æ•™å¸«ãªã— | - | ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹ |
| **ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚¤ãƒ‹ãƒ³ã‚°** | ç›¸é–¢åˆ†æ | - | ãƒ«ãƒ¼ãƒ«æŠ½å‡º |

### 3.2 æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆæ®µéšçš„ï¼‰

#### Phase 1: æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰
```python
# èª¿æ•™çŸ¢å°ã¨ç€é †ã®ç›¸é–¢åˆ†æ
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# èª¿æ•™çŸ¢å°åˆ¥ã®æˆç¸¾é›†è¨ˆ
arrow_stats = df.groupby(['venue', 'course_type', 'training_arrow']).agg({
    'is_top3': 'mean',      # 3ç€ä»¥å†…ç‡
    'finish_position': 'mean',  # å¹³å‡ç€é †
    'count': 'size'
}).reset_index()

# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–
pivot = arrow_stats.pivot_table(
    index='training_arrow', 
    columns=['venue', 'course_type'], 
    values='is_top3'
)
sns.heatmap(pivot, annot=True, cmap='RdYlGn')
```

#### Phase 2: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
```python
# èª¿æ•™é–¢é€£ã®ç‰¹å¾´é‡ä½œæˆ
def create_training_features(df):
    features = {}
    
    # 1. èª¿æ•™çŸ¢å°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    arrow_map = {'â†‘': 2, 'â†—': 1, 'â†’': 0, 'â†˜': -1, 'â†“': -2}
    features['training_arrow_score'] = df['training_arrow'].map(arrow_map)
    
    # 2. çŸ­è©•ã‹ã‚‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    keywords = ['å¥½ä»•ä¸Š', 'ä¸Šæ˜‡', 'å¤‰ã‚ã‚Šèº«', 'å¹³å‡¡', 'ä¸å®‰']
    for kw in keywords:
        features[f'review_has_{kw}'] = df['short_review'].str.contains(kw, na=False).astype(int)
    
    # 3. æ”»ã‚è§£èª¬ã®ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ï¼ˆTF-IDFï¼‰
    from sklearn.feature_extraction.text import TfidfVectorizer
    tfidf = TfidfVectorizer(max_features=50)
    text_features = tfidf.fit_transform(df['attack_explanation'].fillna(''))
    
    # 4. èª¿æ•™å¸«åˆ¥ã®éå»æˆç¸¾
    features['trainer_win_rate'] = df.groupby('trainer_id')['is_win'].transform('mean')
    features['trainer_top3_rate'] = df.groupby('trainer_id')['is_top3'].transform('mean')
    
    # 5. ã‚³ãƒ¼ã‚¹Ã—èª¿æ•™å¸«ã®ç›¸æ€§
    features['trainer_course_rate'] = df.groupby(
        ['trainer_id', 'venue', 'course_type']
    )['is_top3'].transform('mean')
    
    return pd.DataFrame(features)
```

#### Phase 3: ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
```python
from sklearn.model_selection import train_test_split, cross_val_score
from lightgbm import LGBMClassifier
import shap

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    features, target, test_size=0.2, random_state=42
)

# LightGBMãƒ¢ãƒ‡ãƒ«ï¼ˆå‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼‰
model = LGBMClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=6,
    num_leaves=31,
    feature_fraction=0.8,
    bagging_fraction=0.8,
    bagging_freq=5,
    random_state=42
)

# äº¤å·®æ¤œè¨¼
cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')
print(f"CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")

# è¨“ç·´
model.fit(X_train, y_train)

# ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆSHAPå€¤ï¼‰
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test)
```

#### Phase 4: ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
```python
# ã‚³ãƒ¼ã‚¹Ã—èª¿æ•™å¸«Ã—èª¿æ•™è©•ä¾¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
pattern_analysis = df.groupby([
    'venue',           # é–‹å‚¬å ´æ‰€
    'course_type',     # èŠ/ãƒ€ãƒ¼ãƒˆ
    'distance_category', # çŸ­è·é›¢/ä¸­è·é›¢/é•·è·é›¢
    'trainer_id',      # èª¿æ•™å¸«
    'training_arrow'   # èª¿æ•™è©•ä¾¡
]).agg({
    'is_top3': ['mean', 'count'],
    'is_win': 'mean'
}).reset_index()

# çµ±è¨ˆçš„ã«æœ‰æ„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿æŠ½å‡º
pattern_analysis.columns = ['_'.join(col).strip('_') for col in pattern_analysis.columns]
significant_patterns = pattern_analysis[
    (pattern_analysis['is_top3_count'] >= 30) &  # ã‚µãƒ³ãƒ—ãƒ«æ•°30ä»¥ä¸Š
    (pattern_analysis['is_top3_mean'] >= 0.4)    # 3ç€ä»¥å†…ç‡40%ä»¥ä¸Š
]

print("æœ‰åŠ¹ãªèª¿æ•™ãƒ‘ã‚¿ãƒ¼ãƒ³:")
print(significant_patterns.sort_values('is_top3_mean', ascending=False).head(20))
```

---

## 4. æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

### 4.1 é¸æŠè‚¢æ¯”è¼ƒ

| ã‚ªãƒ—ã‚·ãƒ§ãƒ³ | å­¦ç¿’ | æ¨è«– | ã‚³ã‚¹ãƒˆ | æ¨å¥¨åº¦ |
|-----------|------|------|--------|--------|
| **A: ML.NETï¼ˆæ¨å¥¨ï¼‰** | C# | C# | ç„¡æ–™ | â­â­â­ |
| **B: Azure AutoML + C#** | Azure | C# (ONNX) | å¾“é‡èª²é‡‘ | â­â­â­ |
| **C: Python + C#** | Python | C# (ONNX) | ç„¡æ–™ | â­â­ |

### 4.2 æ¨å¥¨æ§‹æˆ: ML.NETï¼ˆC#ã§å®Œçµï¼‰

| ãƒ¬ã‚¤ãƒ¤ãƒ¼ | æŠ€è¡“ | ç”¨é€” |
|---------|------|------|
| **ãƒ‡ãƒ¼ã‚¿å‡¦ç†** | C# + LINQ | å‰å‡¦ç†ãƒ»ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° |
| **æ©Ÿæ¢°å­¦ç¿’** | ML.NET + LightGBM | åˆ†é¡ãƒ»å›å¸°ãƒ¢ãƒ‡ãƒ« |
| **ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ** | ML.NET Tokenizers | æ”»ã‚è§£èª¬ã®NLP |
| **å¯è¦–åŒ–** | Blazor + Chart.js | EDAãƒ»çµæœè¡¨ç¤º |
| **æ¨è«–** | ML.NET ãƒã‚¤ãƒ†ã‚£ãƒ– | é«˜é€Ÿæ¨è«– |
| **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹** | SQL Server + EF Core | ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»é›†è¨ˆ |

### 4.3 NuGetãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼ˆ.NET 10ï¼‰

```xml
<ItemGroup>
  <!-- ML.NET ã‚³ã‚¢ -->
  <PackageReference Include="Microsoft.ML" Version="4.0.0" />
  <!-- LightGBMï¼ˆé«˜æ€§èƒ½å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼‰ -->
  <PackageReference Include="Microsoft.ML.LightGbm" Version="4.0.0" />
  <!-- AutoMLï¼ˆè‡ªå‹•ãƒ¢ãƒ‡ãƒ«é¸æŠï¼‰ -->
  <PackageReference Include="Microsoft.ML.AutoML" Version="0.22.0" />
  <!-- ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆæ”»ã‚è§£èª¬ç”¨ï¼‰ -->
  <PackageReference Include="Microsoft.ML.Tokenizers" Version="0.24.0" />
  <!-- ONNXï¼ˆAzure AutoMLå‡ºåŠ›èª­ã¿è¾¼ã¿ç”¨ï¼‰ -->
  <PackageReference Include="Microsoft.ML.OnnxRuntime" Version="1.18.0" />
</ItemGroup>
```

### 4.4 ML.NETã§ã®å®Ÿè£…ä¾‹

```csharp
using Microsoft.ML;
using Microsoft.ML.Trainers.LightGbm;

public class TrainingPatternTrainer
{
    private readonly MLContext _mlContext = new();
    
    public ITransformer TrainModel(string dataPath)
    {
        // 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        var dataView = _mlContext.Data.LoadFromTextFile<TrainingData>(
            dataPath, separatorChar: ',', hasHeader: true);
        
        // 2. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        var split = _mlContext.Data.TrainTestSplit(dataView, testFraction: 0.2);
        
        // 3. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
        var pipeline = _mlContext.Transforms
            // ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            .Categorical.OneHotEncoding("VenueEncoded", nameof(TrainingData.Venue))
            .Append(_mlContext.Transforms.Categorical.OneHotEncoding(
                "CourseTypeEncoded", nameof(TrainingData.CourseType)))
            .Append(_mlContext.Transforms.Categorical.OneHotEncoding(
                "TrainingArrowEncoded", nameof(TrainingData.TrainingArrow)))
            // ç‰¹å¾´é‡çµåˆ
            .Append(_mlContext.Transforms.Concatenate("Features",
                "VenueEncoded", "CourseTypeEncoded", "TrainingArrowEncoded",
                nameof(TrainingData.Distance), 
                nameof(TrainingData.TrainerWinRate)))
            // LightGBMåˆ†é¡å™¨
            .Append(_mlContext.BinaryClassification.Trainers.LightGbm(
                new LightGbmBinaryTrainer.Options
                {
                    NumberOfLeaves = 31,
                    NumberOfIterations = 500,
                    LearningRate = 0.05f,
                    Deterministic = true
                }));
        
        // 4. è¨“ç·´
        var model = pipeline.Fit(split.TrainSet);
        
        // 5. è©•ä¾¡
        var predictions = model.Transform(split.TestSet);
        var metrics = _mlContext.BinaryClassification.Evaluate(predictions);
        Console.WriteLine($"AUC: {metrics.AreaUnderRocCurve:F4}");
        Console.WriteLine($"Accuracy: {metrics.Accuracy:F4}");
        
        return model;
    }
    
    public void SaveModel(ITransformer model, DataViewSchema schema, string path)
    {
        _mlContext.Model.Save(model, schema, path);
    }
}

// ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
public class TrainingData
{
    [LoadColumn(0)] public string Venue { get; set; } = "";
    [LoadColumn(1)] public string CourseType { get; set; } = "";
    [LoadColumn(2)] public float Distance { get; set; }
    [LoadColumn(3)] public string TrainingArrow { get; set; } = "";
    [LoadColumn(4)] public float TrainerWinRate { get; set; }
    [LoadColumn(5)] public bool IsTop3 { get; set; }
}
```

### 4.5 ML.NET AutoMLï¼ˆè‡ªå‹•ãƒ¢ãƒ‡ãƒ«é¸æŠï¼‰

```csharp
using Microsoft.ML.AutoML;

public class AutoMLTrainer
{
    private readonly MLContext _mlContext = new();
    
    public async Task<ITransformer> TrainWithAutoMLAsync(string dataPath)
    {
        var dataView = _mlContext.Data.LoadFromTextFile<TrainingData>(
            dataPath, separatorChar: ',', hasHeader: true);
        
        var split = _mlContext.Data.TrainTestSplit(dataView, testFraction: 0.2);
        
        // AutoMLè¨­å®š
        var settings = new BinaryClassificationExperimentSettings
        {
            MaxExperimentTimeInSeconds = 600,  // æœ€å¤§10åˆ†
            OptimizingMetric = BinaryClassificationMetric.AreaUnderRocCurve
        };
        
        // AutoMLå®Ÿè¡Œï¼ˆè¤‡æ•°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è‡ªå‹•æ¯”è¼ƒï¼‰
        var experiment = _mlContext.Auto()
            .CreateBinaryClassificationExperiment(settings);
        
        var result = await experiment.ExecuteAsync(
            split.TrainSet, 
            labelColumnName: nameof(TrainingData.IsTop3));
        
        Console.WriteLine($"Best Algorithm: {result.BestRun.TrainerName}");
        Console.WriteLine($"Best AUC: {result.BestRun.ValidationMetrics.AreaUnderRocCurve:F4}");
        
        return result.BestRun.Model;
    }
}
```

### 4.6 Azure AutoMLï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰ç‰ˆï¼‰

æœ€é«˜ç²¾åº¦ã‚’è¿½æ±‚ã™ã‚‹å ´åˆã‚„ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯Azure AutoMLã‚‚é¸æŠè‚¢ï¼š

```csharp
// Azure AutoMLã§è¨“ç·´ã—ãŸONNXãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
using Microsoft.ML.OnnxRuntime;

public class AzureModelPredictor
{
    private readonly InferenceSession _session;
    
    public AzureModelPredictor(string onnxPath)
    {
        _session = new InferenceSession(onnxPath);
    }
    
    public float PredictTop3Probability(float[] features)
    {
        var inputTensor = new DenseTensor<float>(features, new[] { 1, features.Length });
        var inputs = new[] { NamedOnnxValue.CreateFromTensor("input", inputTensor) };
        
        using var results = _session.Run(inputs);
        return results.First().AsTensor<float>()[1];
    }
}
```

### 4.7 Pythonç’°å¢ƒï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³: EDAãƒ»å®Ÿé¨“ç”¨ï¼‰

PythonãŒå¿…è¦ãªå ´åˆã®ã¿ï¼š

```bash
# requirements.txtï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0
lightgbm>=4.0.0
matplotlib>=3.8.0
seaborn>=0.13.0
jupyter>=1.0.0
```

---

## 5. å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### 5.1 ãƒ•ã‚§ãƒ¼ã‚ºè¨ˆç”»

| Phase | æœŸé–“ | å†…å®¹ | æˆæœç‰© |
|-------|------|------|--------|
| **Phase 1** | 1-2é€±é–“ | ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»EDA | åˆ†æãƒ¬ãƒãƒ¼ãƒˆã€å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ |
| **Phase 2** | 2-3é€±é–“ | ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° | ç‰¹å¾´é‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ |
| **Phase 3** | 2-3é€±é–“ | ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ãƒ»è©•ä¾¡ | å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆ.onnxï¼‰ |
| **Phase 4** | 1-2é€±é–“ | ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºãƒ»ãƒ¬ãƒãƒ¼ãƒˆ | ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚«ã‚¿ãƒ­ã‚° |
| **Phase 5** | 1-2é€±é–“ | C#çµ±åˆãƒ»æœ¬ç•ªåŒ– | æ¨è«–API |

### 5.2 Phase 1 è©³ç´°ï¼ˆEDAï¼‰

```python
# Jupyter Notebookæ§‹æˆ
notebooks/
â”œâ”€â”€ 01_data_collection.ipynb      # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»çµåˆ
â”œâ”€â”€ 02_training_analysis.ipynb    # èª¿æ•™ãƒ‡ãƒ¼ã‚¿åˆ†æ
â”œâ”€â”€ 03_course_analysis.ipynb      # ã‚³ãƒ¼ã‚¹åˆ¥åˆ†æ
â”œâ”€â”€ 04_trainer_analysis.ipynb     # èª¿æ•™å¸«åˆ¥åˆ†æ
â””â”€â”€ 05_pattern_discovery.ipynb    # ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹
```

---

## 6. åˆ†æè¦³ç‚¹ã®è©³ç´°

### 6.1 ã‚³ãƒ¼ã‚¹åˆ¥åˆ†æ

| åˆ†æè¦³ç‚¹ | å†…å®¹ | ä»®èª¬ä¾‹ |
|---------|------|--------|
| ã‚³ãƒ¼ã‚¹å½¢æ…‹ | ç›´ç·š/å°å›ã‚Š/å¤§å›ã‚Š | å°å›ã‚Šã‚³ãƒ¼ã‚¹ã§ã¯å‚è·¯èª¿æ•™ãŒæœ‰åŠ¹ |
| è·é›¢ | çŸ­è·é›¢/ä¸­è·é›¢/é•·è·é›¢ | é•·è·é›¢ã§ã¯æŒä¹…åŠ›ç³»èª¿æ•™ãŒé‡è¦ |
| é¦¬å ´ | èŠ/ãƒ€ãƒ¼ãƒˆ | ãƒ€ãƒ¼ãƒˆã¯ãƒ‘ãƒ¯ãƒ¼ç³»èª¿æ•™ |
| å‚ | æ€¥å‚/å¹³å¦ | æ€¥å‚ã‚³ãƒ¼ã‚¹ã§ã¯å‚è·¯èª¿æ•™ |

```python
# ã‚³ãƒ¼ã‚¹åˆ¥èª¿æ•™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
course_patterns = df.groupby([
    'venue', 
    'course_type', 
    'distance_category',
    'training_arrow'
]).apply(lambda x: pd.Series({
    'win_rate': (x['finish_position'] == 1).mean(),
    'top3_rate': (x['finish_position'] <= 3).mean(),
    'avg_position': x['finish_position'].mean(),
    'sample_size': len(x)
})).reset_index()

# æ±äº¬èŠ1600mã§æœ€ã‚‚æœ‰åŠ¹ãªèª¿æ•™ãƒ‘ã‚¿ãƒ¼ãƒ³
tokyo_turf_1600 = course_patterns[
    (course_patterns['venue'] == 'æ±äº¬') & 
    (course_patterns['course_type'] == 'èŠ') &
    (course_patterns['distance_category'] == 'ä¸­è·é›¢')
].sort_values('top3_rate', ascending=False)
```

### 6.2 èª¿æ•™å¸«åˆ¥åˆ†æ

| åˆ†æè¦³ç‚¹ | å†…å®¹ | ä»®èª¬ä¾‹ |
|---------|------|--------|
| ä»•ä¸Šã’ãƒ‘ã‚¿ãƒ¼ãƒ³ | å©ãè‰¯åŒ–/ä¸€ç™ºä»•ä¸Šã’ | èª¿æ•™å¸«Aã¯å©ã„ã¦è‰¯ããªã‚‹ |
| èª¿æ•™å¼·åº¦ | å¼·ã‚/æ™®é€š/è»½ã‚ | èª¿æ•™å¸«Bã®å¼·ã‚èª¿æ•™ã¯ä¿¡é ¼åº¦é«˜ã„ |
| ã‚³ãƒ¼ã‚¹é©æ€§ | å¾—æ„ã‚³ãƒ¼ã‚¹ | èª¿æ•™å¸«Cã¯æ±äº¬ã§æˆç¸¾è‰¯å¥½ |
| èª¿æ•™çŸ¢å°ã®ä¿¡é ¼åº¦ | â†‘ã®å®Ÿéš›ã®æˆç¸¾ | èª¿æ•™å¸«Dã®â†‘ã¯çš„ä¸­ç‡80% |

```python
# èª¿æ•™å¸«åˆ¥ã®èª¿æ•™çŸ¢å°ä¿¡é ¼åº¦
trainer_arrow_reliability = df.groupby(['trainer_id', 'training_arrow']).apply(
    lambda x: pd.Series({
        'predicted_good': len(x[x['training_arrow'].isin(['â†‘', 'â†—'])]),
        'actual_top3': len(x[x['finish_position'] <= 3]),
        'reliability': (x['finish_position'] <= 3).mean() if len(x) > 0 else 0,
        'sample_size': len(x)
    })
).reset_index()

# èª¿æ•™å¸«ã®ã€Œâ†‘è©•ä¾¡ã€ã®ä¿¡é ¼åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
arrow_up_reliability = trainer_arrow_reliability[
    (trainer_arrow_reliability['training_arrow'] == 'â†‘') &
    (trainer_arrow_reliability['sample_size'] >= 20)
].sort_values('reliability', ascending=False)
```

### 6.3 ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆæ”»ã‚è§£èª¬ï¼‰

```python
# æ”»ã‚è§£èª¬ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
from collections import Counter
import re

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
positive_keywords = ['å¥½æ™‚è¨ˆ', 'è»½å¿«', 'æ„æ¬²çš„', 'çµ¶å¥½èª¿', 'ä¸Šæ˜‡', 'å¤‰ã‚ã‚Šèº«']
negative_keywords = ['é‡è‹¦ã—ã„', 'å¹³å‡¡', 'ç‰©è¶³ã‚Šãªã„', 'ä¸å®‰', 'å¤ªã‚']

def extract_sentiment(text):
    if pd.isna(text):
        return 0
    score = 0
    for kw in positive_keywords:
        if kw in text:
            score += 1
    for kw in negative_keywords:
        if kw in text:
            score -= 1
    return score

df['training_sentiment'] = df['attack_explanation'].apply(extract_sentiment)

# ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã¨æˆç¸¾ã®ç›¸é–¢
sentiment_performance = df.groupby('training_sentiment').agg({
    'is_top3': 'mean',
    'finish_position': 'mean',
    'count': 'size'
})
```

---

## 7. è©•ä¾¡æŒ‡æ¨™

### 7.1 ãƒ¢ãƒ‡ãƒ«è©•ä¾¡

| æŒ‡æ¨™ | èª¬æ˜ | ç›®æ¨™å€¤ |
|------|------|--------|
| **AUC-ROC** | åˆ†é¡æ€§èƒ½ã®ç·åˆæŒ‡æ¨™ | 0.65ä»¥ä¸Š |
| **Precision@K** | ä¸Šä½Kä»¶ã®ç²¾åº¦ | 0.40ä»¥ä¸Š |
| **Recall@3** | 3ç€ä»¥å†…é¦¬ã®æ•æ‰ç‡ | 0.50ä»¥ä¸Š |
| **Profit Factor** | å›åç‡ | 1.0ä»¥ä¸Šï¼ˆé»’å­—ï¼‰ |

### 7.2 ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ‰åŠ¹æ€§è©•ä¾¡

```python
# ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š
from scipy import stats

def evaluate_pattern_significance(pattern_df, baseline_rate):
    """
    ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ‰æ„æ€§ã‚’æ¤œå®š
    baseline_rate: å…¨ä½“ã®3ç€ä»¥å†…ç‡ï¼ˆä¾‹: 0.30ï¼‰
    """
    results = []
    for _, row in pattern_df.iterrows():
        n = row['sample_size']
        k = int(row['top3_rate'] * n)
        
        # äºŒé …æ¤œå®š
        p_value = stats.binom_test(k, n, baseline_rate, alternative='greater')
        
        # åŠ¹æœé‡ï¼ˆã‚ªãƒƒã‚ºæ¯”ï¼‰
        odds_ratio = (row['top3_rate'] / (1 - row['top3_rate'])) / \
                     (baseline_rate / (1 - baseline_rate))
        
        results.append({
            'pattern': row['pattern_name'],
            'top3_rate': row['top3_rate'],
            'sample_size': n,
            'p_value': p_value,
            'odds_ratio': odds_ratio,
            'significant': p_value < 0.05
        })
    
    return pd.DataFrame(results)
```

---

## 8. å‡ºåŠ›å½¢å¼

### 8.1 ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚«ã‚¿ãƒ­ã‚°

```json
{
  "pattern_id": "P001",
  "description": "æ±äº¬èŠ1600m Ã— å‚è·¯å¥½æ™‚è¨ˆ Ã— â†‘è©•ä¾¡",
  "conditions": {
    "venue": "æ±äº¬",
    "course_type": "èŠ",
    "distance_range": [1400, 1800],
    "training_arrow": "â†‘",
    "keywords": ["å¥½æ™‚è¨ˆ", "å‚è·¯"]
  },
  "performance": {
    "top3_rate": 0.45,
    "win_rate": 0.18,
    "sample_size": 156,
    "p_value": 0.002,
    "odds_ratio": 1.85
  },
  "recommendation": "å¼·ã„è²·ã„å€™è£œ"
}
```

### 8.2 èª¿æ•™å¸«ä¿¡é ¼åº¦ã‚«ã‚¿ãƒ­ã‚°

```json
{
  "trainer_id": "T0001",
  "trainer_name": "ã€‡ã€‡èª¿æ•™å¸«",
  "specialty": {
    "best_venues": ["æ±äº¬", "ä¸­å±±"],
    "best_course_type": "èŠ",
    "best_distance": "ä¸­è·é›¢"
  },
  "arrow_reliability": {
    "â†‘": { "top3_rate": 0.52, "sample_size": 45 },
    "â†—": { "top3_rate": 0.38, "sample_size": 89 },
    "â†’": { "top3_rate": 0.28, "sample_size": 120 }
  },
  "overall_rating": "A"
}
```

---

## 9. æ³¨æ„äº‹é …ãƒ»ãƒªã‚¹ã‚¯

### 9.1 ãƒ‡ãƒ¼ã‚¿å“è³ª

| ãƒªã‚¹ã‚¯ | å¯¾ç­– |
|--------|------|
| èª¿æ•™ãƒ‡ãƒ¼ã‚¿ã®æ¬ æ | æ¬ æãƒ•ãƒ©ã‚°ã‚’ç‰¹å¾´é‡åŒ– |
| ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨è¨˜æºã‚Œ | æ­£è¦åŒ–ãƒ»é¡ç¾©èªè¾æ›¸ |
| ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³ | æœ€ä½30ä»¶ã®ãƒ•ã‚£ãƒ«ã‚¿ |

### 9.2 éå­¦ç¿’é˜²æ­¢

| ãƒªã‚¹ã‚¯ | å¯¾ç­– |
|--------|------|
| éå­¦ç¿’ | äº¤å·®æ¤œè¨¼ã€æ­£å‰‡åŒ– |
| ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ | æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆæœªæ¥ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ç¦æ­¢ï¼‰ |
| åã‚Š | éšå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |

### 9.3 é‹ç”¨ä¸Šã®æ³¨æ„

- **ç«¶é¦¬ã¯ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„**: é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ã§ã‚‚å¤–ã‚Œã‚‹ã“ã¨ã¯å¤šã„
- **é¦¬åˆ¸è³¼å…¥ã¯è‡ªå·±è²¬ä»»**: äºˆæ¸¬çµæœã‚’éµœå‘‘ã¿ã«ã—ãªã„
- **ç¶™ç¶šçš„ãªæ¤œè¨¼**: ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½åŠ£åŒ–ã‚’ç›£è¦–

---

## 10. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### 10.1 ã™ãã«å§‹ã‚ã‚‰ã‚Œã‚‹ã“ã¨

1. **æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®åé›†**
   - ç«¶é¦¬ãƒ–ãƒƒã‚¯ã®èª¿æ•™ãƒ‡ãƒ¼ã‚¿ï¼ˆCyokyoParserå‡ºåŠ›ï¼‰ã‚’è“„ç©
   - SeisekiParserã§çµæœãƒ‡ãƒ¼ã‚¿ã‚’è“„ç©
   - æœ€ä½3ãƒ¶æœˆåˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒæœ›ã¾ã—ã„

2. **Jupyterç’°å¢ƒæ§‹ç¯‰**
   ```bash
   python -m venv .venv
   .venv\Scripts\activate
   pip install pandas numpy scikit-learn lightgbm matplotlib seaborn jupyter
   jupyter notebook
   ```

3. **æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æé–‹å§‹**
   - èª¿æ•™çŸ¢å°ã¨ç€é †ã®ç›¸é–¢
   - ã‚³ãƒ¼ã‚¹åˆ¥ã®å‹ç‡åˆ†å¸ƒ
   - èª¿æ•™å¸«åˆ¥ã®æˆç¸¾å‚¾å‘

### 10.2 C#ç§»è¡Œå¾Œã«çµ±åˆ

- å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’`.onnx`å½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
- `KeibaCICD.Scraper.Infrastructure`ã«æ¨è«–ã‚µãƒ¼ãƒ“ã‚¹è¿½åŠ 
- MDæ–°èã«ã€ŒAIãƒ‘ã‚¿ãƒ¼ãƒ³è©•ä¾¡ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¿½åŠ 

---

## 11. é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [C#ç§»è¡Œè©³ç´°è¨­è¨ˆæ›¸](./csharp_migration_detailed_design.md)
- [DBçµ±åˆè¨­è¨ˆæ›¸](./database_integration_design.md)
- [ãƒ‘ãƒ¼ã‚µãƒ¼å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒ](./parser_output_schemas.md)
- [IntegrationServiceè¨­è¨ˆ](./integration_service_design.md)
