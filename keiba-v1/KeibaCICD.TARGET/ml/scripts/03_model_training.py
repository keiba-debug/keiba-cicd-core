# -*- coding: utf-8 -*-
"""
ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã¨LightGBMã§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¾ã™ã€‚
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import json
from datetime import datetime

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    roc_auc_score,
    precision_score,
    recall_score,
    f1_score,
    classification_report
)
import lightgbm as lgb
import joblib

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))


def load_data(data_path: Path) -> pd.DataFrame:
    """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {data_path}")
    df = pd.read_csv(data_path)
    print(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")
    return df


def split_data_by_date(
    df: pd.DataFrame,
    split_date: str = "2024-12-31"
) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    æ™‚ç³»åˆ—ã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²

    Args:
        df: DataFrame
        split_date: åˆ†å‰²æ—¥ï¼ˆã“ã®æ—¥ã¾ã§ãŒå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼‰

    Returns:
        train_df, test_df
    """
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—åˆ†å‰²ä¸­ï¼ˆåŸºæº–æ—¥: {split_date}ï¼‰...")

    df = df.sort_values('race_date')

    train_df = df[df['race_date'] <= split_date].copy()
    test_df = df[df['race_date'] > split_date].copy()

    print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(train_df)}ä»¶ ({train_df['race_date'].min()} ï½ {train_df['race_date'].max()})")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_df)}ä»¶ ({test_df['race_date'].min()} ï½ {test_df['race_date'].max()})")

    return train_df, test_df


def prepare_features(
    df: pd.DataFrame,
    feature_cols: list
) -> tuple[pd.DataFrame, pd.Series]:
    """
    ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’åˆ†é›¢

    Args:
        df: DataFrame
        feature_cols: ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆ

    Returns:
        X (ç‰¹å¾´é‡), y (ç›®çš„å¤‰æ•°)
    """
    X = df[feature_cols].fillna(0)
    y = df['target']

    return X, y


def train_logistic_regression(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    X_test: pd.DataFrame,
    y_test: pd.Series
) -> dict:
    """
    ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´

    Returns:
        çµæœè¾æ›¸ï¼ˆãƒ¢ãƒ‡ãƒ«ã€è©•ä¾¡æŒ‡æ¨™ï¼‰
    """
    print("\n=== ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° è¨“ç·´é–‹å§‹ ===")

    model = LogisticRegression(max_iter=1000, random_state=42)
    model.fit(X_train, y_train)

    print("âœ“ è¨“ç·´å®Œäº†")

    # äºˆæ¸¬
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    # è©•ä¾¡
    metrics = {
        'model_name': 'LogisticRegression',
        'accuracy': accuracy_score(y_test, y_pred),
        'auc': roc_auc_score(y_test, y_pred_proba),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred)
    }

    print(f"æ­£è§£ç‡: {metrics['accuracy']:.4f}")
    print(f"AUC: {metrics['auc']:.4f}")
    print(f"é©åˆç‡: {metrics['precision']:.4f}")
    print(f"å†ç¾ç‡: {metrics['recall']:.4f}")
    print(f"F1ã‚¹ã‚³ã‚¢: {metrics['f1']:.4f}")

    # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
    print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(classification_report(y_test, y_pred, target_names=['åœå¤–', 'åœå†…']))

    return {
        'model': model,
        'metrics': metrics,
        'predictions': y_pred_proba
    }


def train_lightgbm(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    X_test: pd.DataFrame,
    y_test: pd.Series,
    params: dict = None
) -> dict:
    """
    LightGBMãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´

    Args:
        X_train, y_train: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
        X_test, y_test: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        params: LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

    Returns:
        çµæœè¾æ›¸ï¼ˆãƒ¢ãƒ‡ãƒ«ã€è©•ä¾¡æŒ‡æ¨™ï¼‰
    """
    print("\n=== LightGBM è¨“ç·´é–‹å§‹ ===")

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    if params is None:
        params = {
            'objective': 'binary',
            'metric': 'auc',
            'boosting': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'min_child_samples': 20,
            'verbose': -1
        }

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    train_data = lgb.Dataset(X_train, label=y_train)
    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

    # è¨“ç·´
    gbm = lgb.train(
        params,
        train_data,
        num_boost_round=1000,
        valid_sets=[test_data],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50),
            lgb.log_evaluation(period=100)
        ]
    )

    print("âœ“ è¨“ç·´å®Œäº†")

    # äºˆæ¸¬
    y_pred_proba = gbm.predict(X_test, num_iteration=gbm.best_iteration)
    y_pred = (y_pred_proba > 0.5).astype(int)

    # è©•ä¾¡
    metrics = {
        'model_name': 'LightGBM',
        'accuracy': accuracy_score(y_test, y_pred),
        'auc': roc_auc_score(y_test, y_pred_proba),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'best_iteration': gbm.best_iteration
    }

    print(f"æ­£è§£ç‡: {metrics['accuracy']:.4f}")
    print(f"AUC: {metrics['auc']:.4f}")
    print(f"é©åˆç‡: {metrics['precision']:.4f}")
    print(f"å†ç¾ç‡: {metrics['recall']:.4f}")
    print(f"F1ã‚¹ã‚³ã‚¢: {metrics['f1']:.4f}")
    print(f"æœ€é©ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {metrics['best_iteration']}")

    # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
    print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(classification_report(y_test, y_pred, target_names=['åœå¤–', 'åœå†…']))

    # ç‰¹å¾´é‡é‡è¦åº¦
    print("\nç‰¹å¾´é‡é‡è¦åº¦ Top 10:")
    importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': gbm.feature_importance(importance_type='gain')
    }).sort_values('importance', ascending=False)

    print(importance.head(10).to_string(index=False))

    return {
        'model': gbm,
        'metrics': metrics,
        'predictions': y_pred_proba,
        'feature_importance': importance
    }


def save_model(
    model,
    metrics: dict,
    feature_cols: list,
    model_dir: Path,
    model_name: str = "lightgbm_model"
):
    """
    ãƒ¢ãƒ‡ãƒ«ã¨é–¢é€£æƒ…å ±ã‚’ä¿å­˜

    Args:
        model: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        metrics: è©•ä¾¡æŒ‡æ¨™
        feature_cols: ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆ
        model_dir: ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        model_name: ãƒ¢ãƒ‡ãƒ«å
    """
    model_dir.mkdir(parents=True, exist_ok=True)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model_path = model_dir / f"{model_name}.pkl"
    joblib.dump(model, model_path)
    print(f"âœ“ ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")

    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ä¿å­˜
    info = {
        'model_name': metrics.get('model_name'),
        'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'features': feature_cols,
        'metrics': metrics,
        'threshold': 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã—ãã„å€¤
    }

    info_path = model_dir / f"{model_name}_info.json"
    with open(info_path, "w", encoding="utf-8") as f:
        json.dump(info, f, ensure_ascii=False, indent=2)
    print(f"âœ“ ãƒ¢ãƒ‡ãƒ«æƒ…å ±ä¿å­˜: {info_path}")


def compare_models(results: dict):
    """
    è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ¯”è¼ƒ

    Args:
        results: {model_name: result_dict}
    """
    print("\n=== ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ ===\n")

    comparison = []
    for name, result in results.items():
        metrics = result['metrics']
        comparison.append({
            'ãƒ¢ãƒ‡ãƒ«': metrics['model_name'],
            'æ­£è§£ç‡': f"{metrics['accuracy']:.4f}",
            'AUC': f"{metrics['auc']:.4f}",
            'é©åˆç‡': f"{metrics['precision']:.4f}",
            'å†ç¾ç‡': f"{metrics['recall']:.4f}",
            'F1': f"{metrics['f1']:.4f}"
        })

    comparison_df = pd.DataFrame(comparison)
    print(comparison_df.to_string(index=False))

    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’åˆ¤å®šï¼ˆAUCã§æ¯”è¼ƒï¼‰
    best_model = max(results.items(), key=lambda x: x[1]['metrics']['auc'])
    print(f"\nğŸ† æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model[1]['metrics']['model_name']} (AUC={best_model[1]['metrics']['auc']:.4f})")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=== ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹ ===\n")

    # ãƒ‘ã‚¹è¨­å®š
    data_dir = project_root / "data"
    model_dir = project_root / "ml" / "models"

    data_path = data_dir / "training" / "race_results_featured.csv"
    feature_cols_path = data_dir / "training" / "feature_columns.txt"

    # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿
    with open(feature_cols_path, "r", encoding="utf-8") as f:
        feature_cols = [line.strip() for line in f.readlines()]

    print(f"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡: {len(feature_cols)}å€‹\n")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_data(data_path)

    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    train_df, test_df = split_data_by_date(df, split_date="2024-12-31")

    # ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’åˆ†é›¢
    X_train, y_train = prepare_features(train_df, feature_cols)
    X_test, y_test = prepare_features(test_df, feature_cols)

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    results = {}

    # 1. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°
    lr_result = train_logistic_regression(X_train, y_train, X_test, y_test)
    results['LogisticRegression'] = lr_result

    # 2. LightGBM
    lgbm_result = train_lightgbm(X_train, y_train, X_test, y_test)
    results['LightGBM'] = lgbm_result

    # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ
    compare_models(results)

    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ï¼ˆAUCãŒæœ€ã‚‚é«˜ã„ãƒ¢ãƒ‡ãƒ«ï¼‰
    best_model_name = max(results.items(), key=lambda x: x[1]['metrics']['auc'])[0]
    best_result = results[best_model_name]

    save_model(
        best_result['model'],
        best_result['metrics'],
        feature_cols,
        model_dir,
        model_name=f"best_model_{best_model_name.lower()}"
    )

    # LightGBMã¯å¸¸ã«ä¿å­˜ï¼ˆå®Ÿé‹ç”¨æƒ³å®šï¼‰
    if 'LightGBM' in results:
        save_model(
            results['LightGBM']['model'],
            results['LightGBM']['metrics'],
            feature_cols,
            model_dir,
            model_name="lightgbm_model"
        )

        # ç‰¹å¾´é‡é‡è¦åº¦ã‚‚ä¿å­˜
        importance_path = model_dir / "feature_importance.csv"
        results['LightGBM']['feature_importance'].to_csv(
            importance_path, index=False, encoding="utf-8-sig"
        )
        print(f"âœ“ ç‰¹å¾´é‡é‡è¦åº¦ä¿å­˜: {importance_path}")

    print("\n=== ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº† ===")


if __name__ == "__main__":
    main()
