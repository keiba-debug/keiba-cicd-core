#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆJSONå°‚ç”¨ãƒ»åŒä¸€ãƒ•ã‚©ãƒ«ãƒ€ä¿å­˜ï¼‰

ç«¶é¦¬ãƒ–ãƒƒã‚¯ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹ã¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import os
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any

from ..scrapers.keibabook_scraper import KeibabookScraper
from ..parsers.seiseki_parser import SeisekiParser
from ..parsers.syutuba_parser import SyutubaParser
from ..parsers.cyokyo_parser import CyokyoParser
from ..parsers.danwa_parser import DanwaParser
from ..parsers.nittei_parser import NitteiParser
from .core.common import ensure_batch_directories, get_race_ids_file_path, get_json_file_path

class DataFetcher:
    """
    ç«¶é¦¬ãƒ–ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¹ï¼ˆJSONå°‚ç”¨ãƒ»åŒä¸€ãƒ•ã‚©ãƒ«ãƒ€ä¿å­˜ï¼‰
    """
    
    def __init__(self, delay: int = 3):
        """
        åˆæœŸåŒ–
        
        Args:
            delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ï¼‰
        """
        self.delay = delay
        self.scraper = KeibabookScraper()
        
        # ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’åˆæœŸåŒ–
        self.seiseki_parser = SeisekiParser()
        self.shutsuba_parser = SyutubaParser()
        self.cyokyo_parser = CyokyoParser()
        self.danwa_parser = DanwaParser()
        self.nittei_parser = NitteiParser()
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        ensure_batch_directories()
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def fetch_race_schedule(self, date_str: str) -> bool:
        """
        æŒ‡å®šæ—¥ã®ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹ã‚’å–å¾—ã—ã¦JSONã§ä¿å­˜
        é–‹å‚¬ãŒãªã„æ—¥ã¯JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›ã—ãªã„
        
        Args:
            date_str: æ—¥ä»˜æ–‡å­—åˆ— (YYYYMMDD)
            
        Returns:
            bool: æˆåŠŸæ™‚Trueï¼ˆé–‹å‚¬ãŒãªã„æ—¥ã‚‚Trueã‚’è¿”ã™ï¼‰
        """
        try:
            self.logger.info(f"ğŸ“… ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹å–å¾—é–‹å§‹: {date_str}")
            
            # ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹ã‚’å–å¾—
            html_content = self.scraper.get_nittei_page(date_str)
            if not html_content:
                self.logger.error(f"âŒ æ—¥ç¨‹ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {date_str}")
                return False
            
            # ãƒ‘ãƒ¼ã‚¹å‡¦ç†ï¼ˆæ—¥ä»˜ã‚’æ¸¡ã™ï¼‰
            parsed_data = self.nittei_parser.parse_with_date(html_content, date_str)
            if not parsed_data:
                self.logger.error(f"âŒ æ—¥ç¨‹ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {date_str}")
                return False
            
            # é–‹å‚¬ãŒãªã„æ—¥ã®åˆ¤å®š
            total_races = parsed_data.get('total_races', 0)
            kaisai_count = parsed_data.get('kaisai_count', 0)
            
            if total_races == 0 or kaisai_count == 0:
                self.logger.info(f"ğŸ“­ é–‹å‚¬ãªã—: {date_str} - ãƒ¬ãƒ¼ã‚¹æ•°: {total_races}, é–‹å‚¬æ•°: {kaisai_count}")
                self.logger.info(f"â­ï¸ JSONãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡ºåŠ›ã—ã¾ã›ã‚“")
                return True  # é–‹å‚¬ãŒãªã„æ—¥ã‚‚æ­£å¸¸å‡¦ç†ã¨ã—ã¦æ‰±ã†
            
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            json_file_path = get_json_file_path('nittei', date_str)
            with open(json_file_path, 'w', encoding='utf-8') as f:
                json.dump(parsed_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"âœ… æ—¥ç¨‹JSONä¿å­˜å®Œäº†: {json_file_path}")
            self.logger.info(f"ğŸ‡ é–‹å‚¬æƒ…å ±: {kaisai_count}é–‹å‚¬, {total_races}ãƒ¬ãƒ¼ã‚¹")
            
            # ãƒ¬ãƒ¼ã‚¹IDæƒ…å ±ã‚‚ä¿å­˜
            race_ids_file = get_race_ids_file_path(date_str)
            with open(race_ids_file, 'w', encoding='utf-8') as f:
                json.dump(parsed_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"âœ… ãƒ¬ãƒ¼ã‚¹IDæƒ…å ±ä¿å­˜å®Œäº†: {race_ids_file}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def save_race_data(self, race_id: str, data_types: List[str]) -> Dict[str, bool]:
        """
        æŒ‡å®šãƒ¬ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦JSONã§ä¿å­˜
        
        Args:
            race_id: ãƒ¬ãƒ¼ã‚¹ID
            data_types: å–å¾—ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            Dict[str, bool]: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã”ã¨ã®æˆåŠŸ/å¤±æ•—
        """
        results = {}
        
        for data_type in data_types:
            try:
                self.logger.info(f"ğŸ“Š {data_type}ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: {race_id}")
                
                # ãƒ‡ãƒ¼ã‚¿å–å¾—
                html_content = None
                if data_type == 'seiseki':
                    html_content = self.scraper.get_seiseki_page(race_id)
                elif data_type == 'shutsuba':
                    html_content = self.scraper.get_shutsuba_page(race_id)
                elif data_type == 'cyokyo':
                    html_content = self.scraper.get_cyokyo_page(race_id)
                elif data_type == 'danwa':
                    html_content = self.scraper.get_danwa_page(race_id)
                
                if not html_content:
                    self.logger.error(f"âŒ {data_type}ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {race_id}")
                    results[data_type] = False
                    continue
                
                # ãƒ‘ãƒ¼ã‚¹å‡¦ç†
                parsed_data = None
                if data_type == 'seiseki':
                    # SeisekiParserã¯ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹è¨­è¨ˆã®ãŸã‚ã€ä¸€æ™‚çš„ã«HTMLã‚’ä¿å­˜
                    import tempfile
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False, encoding='utf-8') as tmp_file:
                        tmp_file.write(html_content)
                        tmp_file_path = tmp_file.name
                    try:
                        parsed_data = self.seiseki_parser.parse(tmp_file_path)
                    finally:
                        import os
                        os.unlink(tmp_file_path)
                elif data_type == 'shutsuba':
                    # SyutubaParserã‚‚ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹è¨­è¨ˆã®ãŸã‚ã€ä¸€æ™‚çš„ã«HTMLã‚’ä¿å­˜
                    import tempfile
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False, encoding='utf-8') as tmp_file:
                        tmp_file.write(html_content)
                        tmp_file_path = tmp_file.name
                    try:
                        parsed_data = self.shutsuba_parser.parse(tmp_file_path)
                    finally:
                        import os
                        os.unlink(tmp_file_path)
                elif data_type == 'cyokyo':
                    # CyokyoParserã‚‚ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹è¨­è¨ˆã®ãŸã‚ã€ä¸€æ™‚çš„ã«HTMLã‚’ä¿å­˜
                    import tempfile
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False, encoding='utf-8') as tmp_file:
                        tmp_file.write(html_content)
                        tmp_file_path = tmp_file.name
                    try:
                        parsed_data = self.cyokyo_parser.parse(tmp_file_path)
                    finally:
                        import os
                        os.unlink(tmp_file_path)
                elif data_type == 'danwa':
                    # DanwaParserã‚‚ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹è¨­è¨ˆã®ãŸã‚ã€ä¸€æ™‚çš„ã«HTMLã‚’ä¿å­˜
                    import tempfile
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False, encoding='utf-8') as tmp_file:
                        tmp_file.write(html_content)
                        tmp_file_path = tmp_file.name
                    try:
                        parsed_data = self.danwa_parser.parse(tmp_file_path)
                    finally:
                        import os
                        os.unlink(tmp_file_path)
                
                if not parsed_data:
                    self.logger.error(f"âŒ {data_type}ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {race_id}")
                    results[data_type] = False
                    continue
                
                # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
                json_file_path = get_json_file_path(data_type, race_id)
                with open(json_file_path, 'w', encoding='utf-8') as f:
                    json.dump(parsed_data, f, ensure_ascii=False, indent=2)
                
                self.logger.info(f"âœ… {data_type} JSONä¿å­˜å®Œäº†: {json_file_path}")
                results[data_type] = True
                
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”
                if self.delay > 0:
                    time.sleep(self.delay)
                    
            except Exception as e:
                self.logger.error(f"âŒ {data_type}ãƒ‡ãƒ¼ã‚¿å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
                results[data_type] = False
        
        return results

    def get_race_ids_from_file(self, date_str: str) -> List[str]:
        """
        ä¿å­˜ã•ã‚ŒãŸãƒ¬ãƒ¼ã‚¹IDæƒ…å ±ã‹ã‚‰ã€ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆã‚’å–å¾—
        
        Args:
            date_str: æ—¥ä»˜æ–‡å­—åˆ— (YYYYMMDD)
            
        Returns:
            List[str]: ãƒ¬ãƒ¼ã‚¹IDã®ãƒªã‚¹ãƒˆ
        """
        try:
            race_ids_file = get_race_ids_file_path(date_str)
            
            if not os.path.exists(race_ids_file):
                self.logger.warning(f"âš ï¸ ãƒ¬ãƒ¼ã‚¹IDæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {race_ids_file}")
                return []
            
            with open(race_ids_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            race_ids = []
            kaisai_data = data.get('kaisai_data', {})
            
            for venue, races in kaisai_data.items():
                for race in races:
                    race_id = race.get('race_id')
                    if race_id:
                        race_ids.append(race_id)
            
            self.logger.info(f"ğŸ“‹ ãƒ¬ãƒ¼ã‚¹IDå–å¾—å®Œäº†: {len(race_ids)}ä»¶")
            return race_ids
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ¬ãƒ¼ã‚¹IDå–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def fetch_all_race_data(self, date_str: str, data_types: List[str]) -> Dict[str, Any]:
        """
        æŒ‡å®šæ—¥ã®å…¨ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        
        Args:
            date_str: æ—¥ä»˜æ–‡å­—åˆ— (YYYYMMDD)
            data_types: å–å¾—ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            Dict[str, Any]: å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼
        """
        self.logger.info(f"ğŸš€ å…¨ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: {date_str}")
        self.logger.info(f"ğŸ“Š å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—: {', '.join(data_types)}")
        
        # ãƒ¬ãƒ¼ã‚¹IDã‚’å–å¾—
        race_ids = self.get_race_ids_from_file(date_str)
        if not race_ids:
            self.logger.error(f"âŒ ãƒ¬ãƒ¼ã‚¹IDãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {date_str}")
            return {'success': False, 'error': 'No race IDs found'}
        
        # å„ãƒ¬ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        total_success = 0
        total_failed = 0
        results_by_type = {data_type: {'success': 0, 'failed': 0} for data_type in data_types}
        
        for i, race_id in enumerate(race_ids, 1):
            self.logger.info(f"ğŸ‡ ãƒ¬ãƒ¼ã‚¹ {i}/{len(race_ids)}: {race_id}")
            
            race_results = self.save_race_data(race_id, data_types)
            
            for data_type, success in race_results.items():
                if success:
                    results_by_type[data_type]['success'] += 1
                    total_success += 1
                else:
                    results_by_type[data_type]['failed'] += 1
                    total_failed += 1
        
        # çµæœã‚µãƒãƒªãƒ¼
        summary = {
            'success': True,
            'date': date_str,
            'total_races': len(race_ids),
            'total_success': total_success,
            'total_failed': total_failed,
            'results_by_type': results_by_type
        }
        
        self.logger.info(f"âœ… å…¨ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")
        self.logger.info(f"ğŸ“Š æˆåŠŸ: {total_success}ä»¶, å¤±æ•—: {total_failed}ä»¶")
        
        return summary 