# 競馬AI開発のためのML学習ロードマップ

松風ブログの知見と競馬AI開発の実務要件から逆算した、学ぶべき知識の体系的整理。

---

## 全体マップ

```
┌─────────────────────────────────────────────────────┐
│                  競馬AIシステム全体像                   │
│                                                       │
│  [データ]→[前処理]→[モデル]→[予測]→[購入判断]→[実行]   │
│     ↑        ↑        ↑       ↑        ↑        ↑     │
│   SQL/DB  特徴量   ML基礎  確率論   期待値    運用    │
│   統計学  エンジニ  アルゴ  キャリ  ケリー基準  自動化  │
│           アリング  リズム  ブレー  ポートフォ         │
│                           ション  リオ理論          │
└─────────────────────────────────────────────────────┘
```

---

## Level 0: 統計・確率の基礎（最優先）

松風ブログの根幹は「確率論的競馬観」。MLアルゴリズムの前にここが土台。

### 0-1. 確率論の基礎

| トピック | なぜ必要か | 松風ブログとの関連 |
|---|---|---|
| 条件付き確率・ベイズの定理 | 「出走馬Aが勝つ確率」の正しい計算 | 003: 確率的予測の基礎 |
| 確率分布（正規、二項、ポアソン） | モデル出力の解釈、分散の理解 | 005: 収束に必要な回数 |
| 大数の法則・中心極限定理 | 「大量購入で収束する」の根拠 | 002: 設定6のジャグラー |
| 期待値と分散 | EV計算、リスク評価の基礎 | 006: EV>1.0を全部買え |

### 0-2. 統計検定の基礎

| トピック | なぜ必要か | 松風ブログとの関連 |
|---|---|---|
| 仮説検定（t検定、カイ二乗検定） | 「回収率100%超は偶然か？」の判定 | 005: 統計的有意性 |
| 信頼区間・ブートストラップ | バックテスト結果の信頼性評価 | 009: 回収率114%の検定 |
| 多重検定補正 | 馬券種別収支の評価 | 012: サイコロ100個の比喩 |
| 自己相関・ラン検定 | 試行間の独立性検証 | 009: 独立性仮定 |

### 0-3. 回帰と相関

| トピック | なぜ必要か |
|---|---|
| 線形回帰・ロジスティック回帰 | 最もシンプルな予測モデル、確率出力の基礎 |
| 相関係数・共分散行列 | 馬券種間の相関分析（ポートフォリオ理論） |
| 決定係数(R²)・残差分析 | モデルの適合度評価 |

---

## Level 1: 機械学習の基礎

### 1-1. 教師あり学習（Supervised Learning）★最重要

競馬AIの中核。「過去データから学習し、未来を予測する」パラダイム。

| トピック | 内容 | 競馬での用途 |
|---|---|---|
| **分類（Classification）** | カテゴリを予測 | 「勝つ/勝たない」の二値分類 |
| **回帰（Regression）** | 連続値を予測 | 走破タイム予測 |
| **確率推定** | 各クラスの確率を出力 | 各馬の勝率推定 ← **本命** |
| **ランキング学習** | 順位関係を学習 | 着順予測 |

#### 目的変数（ターゲット）の設計 ★★★

松風ブログの知見(004)から、**何を予測するか**がアルゴリズム以上に重要。

```
目的変数の候補と特性:

1. 着順（1着/2着/3着...）
   - タイプ: 多クラス分類 or 順序回帰
   - 利点: 直感的
   - 欠点: 「1着を当てる」に最適化されがち → 回収率最大化と乖離

2. 勝率/複勝率（0〜1の確率値）
   - タイプ: 確率推定（二値分類のpredict_proba）
   - 利点: 直接EV計算に使える。松風ブログの思想と完全整合
   - 欠点: キャリブレーション（較正）が必須
   - ★★★ 推奨

3. 走破タイム
   - タイプ: 回帰
   - 利点: 物理的に意味がある、馬場差補正等と組み合わせやすい
   - 欠点: 確率への変換が別途必要

4. 着差/相対タイム
   - タイプ: 回帰
   - 利点: 絶対タイムよりも安定

5. 期待値（EV）を直接予測
   - タイプ: 回帰（オッズを入力に含む）
   - 利点: 最終目標に直結
   - 欠点: オッズ自体が変動する、過学習リスク大
```

#### キャリブレーション（較正）★★

確率推定モデルの生命線。「勝率30%と予測した馬が実際に約30%勝つ」ことの保証。

```
較正が良いモデル:
  予測確率30%の馬 → 実際の勝率 ≈ 30% ← EV計算が正確

較正が悪いモデル:
  予測確率30%の馬 → 実際の勝率 = 15% ← EV計算が狂う → 破産

評価指標:
  - キャリブレーションプロット（信頼度図）
  - ブライアスコア = Σ(予測確率 - 実際結果)² / N
  - 対数損失（Log Loss）

較正手法:
  - Plattスケーリング（シグモイド補正）
  - 等頻度ビニング（Isotonic Regression）
  - Temperature Scaling
```

### 1-2. 教師なし学習（Unsupervised Learning）

直接的には馬券購入に使わないが、データ理解と前処理で有用。

| 手法 | 内容 | 競馬での用途 |
|---|---|---|
| **クラスタリング**（K-means, DBSCAN） | データのグループ分け | レースタイプの分類、馬の脚質分類 |
| **次元削減**（PCA, t-SNE, UMAP） | 高次元データの可視化・圧縮 | 特徴量の可視化、多重共線性の除去 |
| **異常検出** | 外れ値の発見 | 異常オッズ検出、データ品質チェック |

**競馬での優先度**: 低〜中。まず教師あり学習で基盤を作り、必要に応じて導入。

### 1-3. 評価手法 ★★

モデルの良し悪しを正しく測る方法。過学習防止の要。

| トピック | 内容 | なぜ重要か |
|---|---|---|
| **交差検証（Cross-Validation）** | データを分割して汎化性能を測る | 005: 過学習防止の核心 |
| **時系列分割** | 未来のデータでテスト | 競馬は時系列データ → ランダム分割はNG |
| **学習曲線** | サンプル数と性能の関係 | 十分なデータ量の判断 |
| **混同行列・AUC-ROC** | 分類性能の多角的評価 | 的中率以外の評価軸 |

```
★★★ 競馬での交差検証の注意点:

  × ランダムにtrain/testを分割
    → 未来のデータで学習してしまう（データリーク）

  ○ 時系列で分割（Walk-Forward Validation）
    → 2020年データで学習 → 2021年データでテスト
    → 2020-2021年データで学習 → 2022年データでテスト
    → ...
```

---

## Level 2: アルゴリズム詳論

### 2-1. アンサンブル学習 ★★

複数のモデルを組み合わせて性能を上げる手法群。

```
アンサンブルの種類:

1. バギング（Bagging）
   代表: RandomForest
   原理: 複数の決定木を並列に学習 → 多数決/平均
   利点: 過学習に強い、安定、チューニング少

2. ブースティング（Boosting）★★★
   代表: LightGBM, XGBoost, CatBoost
   原理: 弱い学習器を逐次的に改善 → 残差を学習
   利点: 表形式データで最強クラス、高精度

3. スタッキング（Stacking）
   原理: 複数モデルの出力を別モデルの入力にする
   利点: 異なるモデルの長所を統合
   リスク: 過学習しやすい、複雑化
```

### 2-2. 勾配ブースティング（Gradient Boosting）★★★

**競馬AI開発のデファクトスタンダード**。松風もおそらくこの系統。

| ライブラリ | 特徴 | 推奨度 |
|---|---|---|
| **LightGBM** | 高速、メモリ効率良、カテゴリ変数を直接扱える | ★★★ 最推奨 |
| **XGBoost** | 安定、実績豊富 | ★★ |
| **CatBoost** | カテゴリ変数に強い、順序ブースティング | ★★ |

```
なぜ勾配ブースティングか:
  1. 表形式データ（CSV）で最高性能（← 競馬データは表形式）
  2. 特徴量重要度が見える → どの情報が効いているか分かる
  3. 欠損値を自然に扱える
  4. カテゴリ変数をエンコードなしで扱える（LightGBM, CatBoost）
  5. チューニングの余地が大きい
  6. 確率出力（predict_proba）が可能

学ぶべき主要パラメータ:
  - learning_rate: 学習率（小さいほど慎重、大きいほど速い）
  - num_leaves / max_depth: 木の複雑さ
  - n_estimators: 木の数
  - min_child_samples: 葉の最小サンプル数（過学習防止）
  - reg_alpha / reg_lambda: 正則化（L1/L2）
  - subsample / colsample: ランダム化（過学習防止）
```

### 2-3. ディープラーニング（Deep Learning）

| 観点 | 評価 |
|---|---|
| 表形式データへの適用 | △ 勾配ブースティングに劣ることが多い |
| 画像データ（レース映像等） | ★★★ 圧倒的に有利 |
| テキストデータ（競馬新聞コメント等） | ★★ 有用 |
| 学習コスト（人間の学習） | 高い（基礎知識が多い） |
| 計算コスト | 高い（GPU必要） |

```
ディープラーニングが活きる場面:

  1. レース映像解析（CNN / Vision Transformer）
     → 位置取り、脚質、不利の自動検出
     → 012: 松風が数千万円投資した「目視データ」の自動化版

  2. テキスト情報の活用（NLP / Transformer）
     → 競馬新聞のコメント、調教師コメントの数値化

  3. 時系列パターン（RNN / LSTM / Transformer）
     → 過去N走の成績パターンの学習

  4. TabNet等の表形式特化DL
     → 勾配ブースティングの代替として検討可能
```

**結論**: まずLightGBMで基盤を作る。ディープラーニングは独自データ（映像等）に拡張する際に学ぶ。

### 2-4. アルゴリズム選択の判断基準

```
判断フロー:

  データは表形式？
    ├─ Yes → LightGBM / XGBoost（★★★ まずこれ）
    │         └─ 確率出力が不十分 → キャリブレーション追加
    │
    └─ No
        ├─ 画像 → CNN / Vision Transformer
        ├─ テキスト → BERT / Transformer
        └─ 時系列 → LSTM / Transformer

  松風(004)の「アルゴリズムは何でもいい」:
    → 表形式データにおいて、LightGBMとXGBoostの差は
      特徴量設計の差に比べて小さい、という意味
```

---

## Level 3: 特徴量エンジニアリング ★★★

松風ブログで「アルゴリズムより重要」と断言された領域。

### 3-1. 特徴量の種類

| カテゴリ | 具体例 | エンコーディング |
|---|---|---|
| **数値** | 走破タイム、馬体重、オッズ | そのまま or 正規化 |
| **カテゴリ** | 競馬場、馬場状態、脚質 | Label / OneHot / Target Encoding |
| **順序** | クラス（G1>G2>...）、枠番 | 数値マッピング |
| **時系列** | 過去N走の成績 | 集約統計量（平均、最大、トレンド） |
| **テキスト** | 調教コメント | TF-IDF / Embedding |

### 3-2. 競馬特有の特徴量設計

```
基本特徴量（公開データから）:
  - 馬の基本情報: 年齢、性別、馬体重、馬体重増減
  - 過去成績: 過去N走の着順、タイム、上がり3F
  - コース適性: 芝/ダート、距離、右/左回り
  - 騎手: 騎手ID、騎手の直近成績
  - 調教: 調教タイム、調教パターン
  - 枠順: 枠番、馬番
  - レース条件: クラス、出走頭数、天候、馬場状態

上級特徴量（独自計算）:
  - オッズ乖離度: AI予測確率 vs 市場オッズ（001）
  - スマートマネー指標: 最終→確定オッズの変動（007）
  - 馬主クラブフラグ: 応援馬券によるオッズ歪み（001）
  - 各馬券のEV: 確率 × オッズ（006）

差別化特徴量（独自データから）:
  - レース映像解析: 位置取り、不利、コース取り（012）
  - パドック解析: 馬体のコンディション
  - 馬場の詳細: 内外差、含水率
```

### 3-3. 特徴量選択と注意点

```
やるべきこと:
  ✓ 特徴量重要度の確認（LightGBMのfeature_importance）
  ✓ 相関の高い特徴量の除去（多重共線性）
  ✓ 時間的にリークしない特徴量のみ使用
  ✓ 十分なサンプルがある粒度で集計

やってはいけないこと（005の教訓）:
  × 種牡馬×距離×コースの細分化分析
  × 少数サンプルで高回収率の条件を探す
  × 結果を見てから特徴量を追加/削除
```

---

## Level 4: 購入戦略の数理

### 4-1. 期待値（EV）計算

```
基本: EV = P(的中) × オッズ
  EV > 1.0 → 長期的にプラス → 購入
  EV < 1.0 → 長期的にマイナス → 見送り
```

### 4-2. ケリー基準（Kelly Criterion）

最適な賭け金を数学的に導出する理論。

```
ケリー基準:
  f* = (p × b - q) / b

  f* = 最適賭け率（バンクロールに対する割合）
  p  = 勝率
  q  = 1 - p（負ける確率）
  b  = オッズ - 1（純利益倍率）

例: 勝率30%、オッズ5.0の場合
  f* = (0.3 × 4 - 0.7) / 4 = 0.125（バンクの12.5%）

実用上の注意:
  - フルケリーは変動が激しすぎる
  - ハーフケリー（f*/2）or クォーターケリーが推奨
  - 松風の「safety_factor」はこの調整に相当
```

### 4-3. ポートフォリオ理論

```
マーコウィッツ理論の馬券版:
  目的: 期待リターンを最大化しつつ、リスク（分散）を最小化
  手段: 相関の低い馬券種を組み合わせる

  シャープレシオ = (期待リターン - 無リスク利子率) / 標準偏差
  → 馬券では: (回収率 - 1) / 収支の標準偏差
```

### 4-4. モンテカルロシミュレーション

```
用途:
  - 破産確率の推定
  - ドローダウン分布の把握
  - パラメータの最適値探索

手順:
  1. モデルの的中率・回収率分布を推定
  2. N年分の仮想レースを生成
  3. バンクロール推移をシミュレーション
  4. 破産（バンク < 閾値）した回数 / 総試行回数 = 破産確率
  5. 10万回以上繰り返して信頼性を確保
```

---

## Level 5: 実装・運用スキル

### 5-1. Pythonライブラリ

| 領域 | ライブラリ | 用途 |
|---|---|---|
| データ操作 | pandas, numpy | データ前処理の基盤 |
| ML | scikit-learn | 基本ML、前処理、評価 |
| 勾配ブースティング | lightgbm, xgboost | メインモデル |
| 可視化 | matplotlib, seaborn | キャリブレーションプロット等 |
| 統計 | scipy.stats | 検定、分布 |
| DL（将来） | PyTorch | 画像・テキスト解析 |

### 5-2. データベース

```
必要な知識:
  - SQL基礎（SELECT, JOIN, GROUP BY, WINDOW関数）
  - JRA-VANデータのスキーマ理解
  - 時系列データの効率的なクエリ
```

### 5-3. 自動化・運用

```
必要な知識:
  - スケジューリング（cron, タスクスケジューラ）
  - プロセス管理（マルチプロセス、状態同期 ← 011のバグ教訓）
  - エラーハンドリング・リトライ
  - ログ管理（改ざん防止 ← 010の税務要件）
```

---

## 学習の優先順位

```
Phase 1（まず動くものを作る）:
  ├── 確率・統計の基礎（Level 0）
  ├── 教師あり学習の基礎（Level 1-1）
  ├── LightGBMの使い方（Level 2-2）
  ├── 基本特徴量の設計（Level 3）
  └── EV計算の実装（Level 4-1）

Phase 2（精度を上げる）:
  ├── キャリブレーション（Level 1-1）
  ├── 交差検証・時系列分割（Level 1-3）
  ├── 特徴量エンジニアリング応用（Level 3）
  ├── 過学習対策の深掘り（Level 1-3, 2-2）
  └── ケリー基準・バンクロール管理（Level 4-2）

Phase 3（差別化する）:
  ├── ポートフォリオ最適化（Level 4-3）
  ├── モンテカルロシミュレーション（Level 4-4）
  ├── ディープラーニング基礎（Level 2-3）
  ├── 独自データの構築（映像解析等）
  └── 運用自動化（Level 5）
```

---

## 松風ブログの知見 × ML知識の対応表

| 松風ブログの知見 | 必要なML知識 |
|---|---|
| 確率論的競馬観（003） | 確率推定、キャリブレーション |
| EV>1.0を全部買え（006） | 期待値計算、ケリー基準 |
| アルゴリズムは何でもいい（004） | 勾配ブースティング（まずLightGBM） |
| 特徴量が全て（004） | 特徴量エンジニアリング |
| 過学習の回避（005） | 交差検証、正則化、時系列分割 |
| 馬券ポートフォリオ（008） | 相関行列、ポートフォリオ理論 |
| スマートマネー（007） | 時系列分析、オッズ変動モデル |
| 独立性仮定（009） | 自己相関、ラン検定、マルコフ連鎖 |
| 破産確率（008,009） | モンテカルロシミュレーション |
| 独自データ（012） | ディープラーニング（CNN, NLP） |
| Themis設計（011） | ソフトウェア設計、モジュール分離 |

---

## タグ

`#モデル` `#特徴量` `#確率論` `#過学習` `#馬券戦略` `#バンクロール`
