#!/usr/bin/env python3
"""
å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—ã—ã¦umacdã®æ§‹é€ ã‚’åˆ†æã™ã‚‹ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

from src.keibabook.scrapers.requests_scraper import RequestsScraper
from bs4 import BeautifulSoup
import os
import re

def main():
    # RequestsScraperã§HTMLã‚’å–å¾—
    scraper = RequestsScraper()
    
    # å…ˆã»ã©å–å¾—ã—ãŸãƒ¬ãƒ¼ã‚¹IDã‚’ä½¿ç”¨ï¼ˆæ±äº¬1Rï¼‰
    race_id = '202503040101'
    url = f'https://p.keibabook.co.jp/cyuou/syutuba/{race_id}'
    
    print(f"ğŸ” å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­: {url}")
    html_content = scraper.scrape(url)
    
    # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    os.makedirs('debug_html', exist_ok=True)
    html_file = f'debug_html/syutuba_{race_id}.html'
    with open(html_file, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"âœ… HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: {html_file}")
    print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {len(html_content):,}æ–‡å­—")
    
    # BeautifulSoupã§è§£æ
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # umacdã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’æ¢ã™
    print("\nğŸ” umacdå±æ€§ã‚’æŒã¤è¦ç´ ã‚’åˆ†æä¸­...")
    
    umacd_links = soup.find_all('a', attrs={'umacd': True})
    print(f"ğŸ“‹ umacdå±æ€§ã‚’æŒã¤ãƒªãƒ³ã‚¯æ•°: {len(umacd_links)}")
    
    for i, link in enumerate(umacd_links[:10]):  # æœ€åˆã®10å€‹ã‚’è¡¨ç¤º
        umacd = link.get('umacd')
        href = link.get('href', '')
        text = link.get_text(strip=True)
        class_attr = link.get('class', [])
        
        print(f"   [{i+1}] umacd='{umacd}' | text='{text}' | href='{href}' | class={class_attr}")
    
    # é¦¬åã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’æ¢ã™
    print("\nğŸ é¦¬åãƒªãƒ³ã‚¯ã®æ§‹é€ ã‚’åˆ†æä¸­...")
    
    # é¦¬åã‚‰ã—ã„ãƒªãƒ³ã‚¯ã‚’æ¢ã™ï¼ˆumalinkã‚¯ãƒ©ã‚¹ã‚„db/umaã‚’å«ã‚€hrefï¼‰
    horse_links = soup.find_all('a', href=re.compile(r'/db/uma/'))
    print(f"ğŸ“‹ /db/uma/ã‚’å«ã‚€ãƒªãƒ³ã‚¯æ•°: {len(horse_links)}")
    
    for i, link in enumerate(horse_links[:5]):  # æœ€åˆã®5å€‹ã‚’è¡¨ç¤º
        umacd = link.get('umacd')
        href = link.get('href', '')
        text = link.get_text(strip=True)
        class_attr = link.get('class', [])
        
        print(f"   [{i+1}] umacd='{umacd}' | text='{text}' | href='{href}' | class={class_attr}")
    
    # å‡ºé¦¬è¡¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
    print("\nğŸ“Š å‡ºé¦¬è¡¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹é€ ã‚’åˆ†æä¸­...")
    
    tables = soup.find_all('table')
    print(f"ğŸ“‹ å…¨ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")
    
    for i, table in enumerate(tables):
        # ãƒ†ãƒ¼ãƒ–ãƒ«å†…ã®umacdãƒªãƒ³ã‚¯ã‚’ç¢ºèª
        table_umacd_links = table.find_all('a', attrs={'umacd': True})
        if table_umacd_links:
            print(f"\nğŸ ãƒ†ãƒ¼ãƒ–ãƒ« {i+1} (umacdãƒªãƒ³ã‚¯: {len(table_umacd_links)}å€‹):")
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã®æœ€åˆã®æ•°è¡Œã‚’è¡¨ç¤º
            rows = table.find_all('tr')[:5]
            for j, row in enumerate(rows):
                cells = row.find_all(['th', 'td'])
                cell_info = []
                
                for cell in cells:
                    cell_text = cell.get_text(strip=True)[:30]  # æœ€åˆã®30æ–‡å­—
                    umacd_link = cell.find('a', attrs={'umacd': True})
                    if umacd_link:
                        umacd = umacd_link.get('umacd')
                        cell_info.append(f"{cell_text} [umacd:{umacd}]")
                    else:
                        cell_info.append(cell_text)
                
                print(f"   è¡Œ{j+1}: {cell_info}")
    
    scraper.close()

if __name__ == '__main__':
    main() 